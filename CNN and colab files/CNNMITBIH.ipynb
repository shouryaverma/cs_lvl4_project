{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNMITBIH.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP7hml7VOHV5",
        "outputId": "100ca14e-9f31-4e6e-dace-349143a03fda"
      },
      "source": [
        "!pip install eli5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: eli5 in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.11.2)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (20.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->eli5) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA0S5EmRpH7J"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "from scipy.stats.stats import kendalltau\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import log_loss\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import LSTM, Masking\n",
        "from tensorflow.keras.layers import MaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras import activations\n",
        "sns.set()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4at-1lngSJ-",
        "outputId": "0c098f42-68fa-42a5-d3ad-f1f07596e213"
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import *\n",
        "\n",
        "train_values = np.empty(shape=[0, 222])\n",
        "test_values = np.empty(shape=[0, 222])\n",
        "\n",
        "train_sampled_all = glob.glob('./drive/MyDrive/compsci/train_sampled_all_220.csv')\n",
        "# test_sampled_all = glob.glob('./drive/MyDrive/compsci/test_sampled_all_centered_220.csv')\n",
        "test_unsampled_all = glob.glob('./drive/MyDrive/compsci/test_unsampled_all_220.csv')\n",
        "\n",
        "for j in train_sampled_all:\n",
        "    print('Loading ', j)\n",
        "    csvrows = np.loadtxt(j, delimiter=',')\n",
        "    train_values = np.append(train_values, csvrows, axis=0)\n",
        "    \n",
        "# for j in test_all:\n",
        "#     print('Loading ', j)\n",
        "#     csvrows = np.loadtxt(j, delimiter=',')\n",
        "#     test_values = np.append(test_values, csvrows, axis=0)\n",
        "\n",
        "for j in test_unsampled_all:\n",
        "    print('Loading ', j)\n",
        "    csvrows = np.loadtxt(j, delimiter=',')\n",
        "    test_values = np.append(test_values, csvrows, axis=0)\n",
        "    \n",
        "print(train_values.shape)\n",
        "print(test_values.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading  ./drive/MyDrive/compsci/train_sampled_all_220.csv\n",
            "Loading  ./drive/MyDrive/compsci/test_unsampled_all_220.csv\n",
            "(31912, 222)\n",
            "(37863, 222)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23J4cnrOkIAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1a7fde-6aa5-4e99-c559-601db1cda633"
      },
      "source": [
        "X_train = train_values[:,:-2]\n",
        "X_test = test_values[:,:-2]\n",
        "\n",
        "y_train = train_values[:,-2]\n",
        "y_test = test_values[:,-2]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "y_train_gc = (y_train - y_train.min())/(y_train.max()-y_train.min())*(9-1)\n",
        "y_test_gc = (y_test - y_test.min())/(y_test.max()-y_test.min())*(9-1)\n",
        "print(y_train_gc.shape)\n",
        "print(y_test_gc.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31912, 220)\n",
            "(37863, 220)\n",
            "(31912,)\n",
            "(37863,)\n",
            "(31912,)\n",
            "(37863,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Zs07qB4F7n"
      },
      "source": [
        "# # X_train = X_train.reshape(-1, X_train.shape[1],1)\n",
        "# # X_test = X_test.reshape(-1, X_test.shape[1],1)\n",
        "\n",
        "# X_train1 = X_train.reshape(X_train.shape + (1,1))\n",
        "# X_test1 = X_test.reshape(X_test.shape + (1,1))\n",
        "# print(X_train1.shape)\n",
        "# print(X_test1.shape)\n",
        "\n",
        "# y_train1=to_categorical(y_train)\n",
        "# y_test1=to_categorical(y_test)\n",
        "# print(y_train1.shape)\n",
        "# print(y_test1.shape)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwpURVTtJgJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c91defb-bae3-437a-cc67-8b2793048a89"
      },
      "source": [
        "N_train = train_values\r\n",
        "N_test = test_values[test_values[:,-2]==2]\r\n",
        "print(N_train.shape)\r\n",
        "print(N_test.shape)\r\n",
        "\r\n",
        "X_train = N_train[:,:-2]\r\n",
        "X_test = N_test[:,:-2]\r\n",
        "\r\n",
        "y_train = train_values[:len(N_train),-2]\r\n",
        "y_test = test_values[:len(N_test),-2]\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)\r\n",
        "print(y_train.shape)\r\n",
        "print(y_test.shape)\r\n",
        "\r\n",
        "X_train1 = X_train.reshape(X_train.shape + (1,1,))\r\n",
        "X_test1 = X_test.reshape(X_test.shape + (1,1,))\r\n",
        "print(X_train1.shape)\r\n",
        "print(X_test1.shape)\r\n",
        "\r\n",
        "y_train1=to_categorical(y_train)\r\n",
        "y_test1=to_categorical(y_test)\r\n",
        "print(y_train1.shape)\r\n",
        "print(y_test1.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31912, 222)\n",
            "(1660, 222)\n",
            "(31912, 220)\n",
            "(1660, 220)\n",
            "(31912,)\n",
            "(1660,)\n",
            "(31912, 220, 1, 1)\n",
            "(1660, 220, 1, 1)\n",
            "(31912, 9)\n",
            "(1660, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkIBFaGsoxlA"
      },
      "source": [
        "def showResults(test, pred):\n",
        "    #target_names = ['positive', 'negative']\n",
        "    # print(classification_report(test, pred, target_names=target_names))\n",
        "    accuracy = accuracy_score(test, pred)\n",
        "    precision=precision_score(test, pred, average='weighted')\n",
        "    f1Score=f1_score(test, pred, average='weighted') \n",
        "    #loss=log_loss(test,pred)\n",
        "    print(\"Accuracy  : {}\".format(accuracy))\n",
        "    print(\"Precision : {}\".format(precision))\n",
        "    print(\"f1Score : {}\".format(f1Score))\n",
        "    #print(\"Loss : {}\".format(loss))\n",
        "    cm=confusion_matrix(test, pred)\n",
        "    print(cm)\n",
        "    return cm"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8_TEAPnXhDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f91436-c528-4652-9628-5c056199d0d4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n",
        "\r\n",
        "verbose, epoch, batch_size = 1, 5, 256\r\n",
        "activationFunction='relu'\r\n",
        "\r\n",
        "def getlstmModel():\r\n",
        "    \r\n",
        "    lstmmodel = Sequential()\r\n",
        "    lstmmodel.add(LSTM(128, return_sequences=True, input_shape=(X_train1.shape[1],1)))\r\n",
        "    lstmmodel.add(LSTM(9, return_sequences=True))\r\n",
        "    lstmmodel.add(MaxPooling1D(pool_size=2))\r\n",
        "    lstmmodel.add(Flatten())\r\n",
        "    lstmmodel.add(Dense(512, activation=tf.nn.relu))    \r\n",
        "    lstmmodel.add(Dense(128, activation=tf.nn.relu))    \r\n",
        "    lstmmodel.add(Dense(32, activation=tf.nn.relu))\r\n",
        "    lstmmodel.add(Dense(9, activation='softmax'))\r\n",
        "    lstmmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\r\n",
        "    lstmmodel.summary()\r\n",
        "    return lstmmodel\r\n",
        "\r\n",
        "lstmmodel = getlstmModel()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 220, 128)          66560     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 220, 9)            4968      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 110, 9)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 990)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               507392    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 9)                 297       \n",
            "=================================================================\n",
            "Total params: 649,009\n",
            "Trainable params: 649,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q4V12luY8v4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5374bd35-9026-479f-e292-385bf1e660e5"
      },
      "source": [
        "lstmmodelhistory= lstmmodel.fit(X_train1[:,:,:,0], y_train1, epochs=epoch, verbose=verbose, validation_split=0.2, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25529 samples, validate on 6383 samples\n",
            "Epoch 1/5\n",
            "25529/25529 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.7991"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25529/25529 [==============================] - 47s 2ms/sample - loss: 0.6458 - accuracy: 0.7991 - val_loss: 0.2518 - val_accuracy: 0.9259\n",
            "Epoch 2/5\n",
            "25529/25529 [==============================] - 47s 2ms/sample - loss: 0.1929 - accuracy: 0.9407 - val_loss: 0.1865 - val_accuracy: 0.9414\n",
            "Epoch 3/5\n",
            "25529/25529 [==============================] - 46s 2ms/sample - loss: 0.1357 - accuracy: 0.9561 - val_loss: 0.1472 - val_accuracy: 0.9547\n",
            "Epoch 4/5\n",
            "14336/25529 [===============>..............] - ETA: 18s - loss: 0.1054 - accuracy: 0.9672"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke1N616UabPi"
      },
      "source": [
        "############## Get CAM ################\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# from matplotlib.backends.backend_pdf import PdfPages\r\n",
        "\r\n",
        "get_last_conv1 = keras.backend.function([lstmmodel.layers[0].input, keras.backend.learning_phase()], [lstmmodel.layers[1].output])\r\n",
        "last_conv1_lstm = get_last_conv1([X_test1[:1000,:,:,0]])[0]\r\n",
        "\r\n",
        "get_softmax1_lstm = keras.backend.function([lstmmodel.layers[0].input, keras.backend.learning_phase()], [lstmmodel.layers[-1].output])\r\n",
        "softmax1_lstm = get_softmax1_lstm(([X_test1[:1000,:,:,0]]))[0]\r\n",
        "softmax_weight1_lstm = lstmmodel.get_weights()[-1]\r\n",
        "softmax_weight_lstm = np.reshape(softmax_weight1_lstm,(9,1))\r\n",
        "\r\n",
        "CAM = np.dot(last_conv1_lstm, softmax_weight_lstm)\r\n",
        "\r\n",
        "# pp = PdfPages('CAM.pdf')\r\n",
        "for k in range(1):\r\n",
        "    CAM = (CAM - CAM.min(axis=1, keepdims=True)) / (CAM.max(axis=1, keepdims=True) - CAM.min(axis=1, keepdims=True))\r\n",
        "    c = np.exp(CAM) / np.sum(np.exp(CAM), axis=1, keepdims=True)\r\n",
        "    plt.figure(figsize=(18, 4))\r\n",
        "    plt.plot(X_test1[k].squeeze())\r\n",
        "    plt.scatter(np.arange(len(X_test1[k])), X_test1[k].squeeze(), cmap='inferno_r', c=c[k,:].squeeze(), s=50)\r\n",
        "    plt.title('True label:' + str(N_test[k,-2]) + '   likelihood of label ' + str(N_test[k,-2]) + ': ' + str(softmax1_lstm[k][int(y_test[k])]))\r\n",
        "    # plt.title('True label:' + str(y_test[k]) + '   likelihood of label ' + str(y_test[k]) + ': ' + str(softmax1_lstm[k][int(y_test[k])]))\r\n",
        "    plt.clim(0.003,0.010)\r\n",
        "    plt.colorbar()\r\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB2sSnfSZJVX"
      },
      "source": [
        "lstmpredictions = lstmmodel.predict(X_test1[:,:,:,0], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW7h2mcUZPbQ"
      },
      "source": [
        "lstm_predict=np.argmax(lstmpredictions,axis=1)\r\n",
        "lstm_actual_value=np.argmax(y_test1,axis=1)\r\n",
        "lstm_cf_m = showResults(lstm_actual_value, lstm_predict)\r\n",
        "from sklearn import metrics\r\n",
        "lstmmetrics = metrics.classification_report(lstm_actual_value, lstm_predict, digits=3)\r\n",
        "print(lstmmetrics, 'lstm metrics')\r\n",
        "categories=['N','L','R','V','A','F','f','/']\r\n",
        "plt.figure(figsize=(8,6))\r\n",
        "lstm_cf_m = lstm_cf_m.astype('float')/ lstm_cf_m.sum(axis=1)[:,np.newaxis]\r\n",
        "sns.heatmap(lstm_cf_m,annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)\r\n",
        "plt.title('LSTM confusion matrix')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu63p7RT_ER-"
      },
      "source": [
        "from itertools import islice\r\n",
        "def means_of_slices(iterable, slice_size):\r\n",
        "    iterator = iter(iterable)\r\n",
        "    while True:\r\n",
        "        slice = list(islice(iterator, slice_size))\r\n",
        "        if slice:\r\n",
        "            yield np.sum(slice)/len(slice)\r\n",
        "        else:\r\n",
        "            return\r\n",
        "a = last_conv1_lstm\r\n",
        "new_last_lstm = []\r\n",
        "for i in range(len(last_conv1_lstm)):\r\n",
        "  means = list(means_of_slices(a[i], 20))\r\n",
        "  new_last_lstm.append(means)\r\n",
        "new_last_lstm = np.array(new_last_lstm)\r\n",
        "print(new_last_lstm.shape)\r\n",
        "\r\n",
        "final_last_lstm = []\r\n",
        "for i in new_last_lstm:\r\n",
        "  final_last_lstm.append(np.repeat(i,20))\r\n",
        "final_last_lstm = np.array(final_last_lstm)\r\n",
        "print(final_last_lstm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brVTO_HVQB0B"
      },
      "source": [
        "import eli5\r\n",
        "from eli5.sklearn import PermutationImportance\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "\r\n",
        "NNMLP_clf1 = MLPClassifier(random_state=48, max_iter=50)\r\n",
        "NNMLP_clf1.fit(new_last_lstm, y_test1[:1000])\r\n",
        "perm = PermutationImportance(NNMLP_clf1).fit(new_last_lstm, y_test1[:1000])\r\n",
        "print('LSTM result')\r\n",
        "eli5.show_weights(perm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtfXE7nS2xKQ"
      },
      "source": [
        "conv_df_lstm = pd.DataFrame(new_last_lstm)\r\n",
        "corr_lstm = conv_df_lstm.corr(method='kendall')\r\n",
        "slices_nums = [1,2,3,4,5,6,7,8,9,10,11]\r\n",
        "rcParams['figure.figsize'] = 8,6\r\n",
        "sns.heatmap(corr_lstm,annot=True,xticklabels=slices_nums,yticklabels=slices_nums,vmin=-0.6,vmax=1,fmt='0.1g')\r\n",
        "plt.title('LSTM Kendall Tau Corellation')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMlg2UkIBksT"
      },
      "source": [
        "############## Get CAM ################\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "CAM = np.dot(last_conv1_lstm, softmax_weight_lstm)\r\n",
        "\r\n",
        "# pp = PdfPages('CAM.pdf')\r\n",
        "for k in range(20):\r\n",
        "    CAM = (CAM - CAM.min(axis=1, keepdims=True)) / (CAM.max(axis=1, keepdims=True) - CAM.min(axis=1, keepdims=True))\r\n",
        "    c = np.exp(CAM) / np.sum(np.exp(CAM), axis=1, keepdims=True)\r\n",
        "    plt.figure(figsize=(18, 4))\r\n",
        "    plt.plot(X_test1[k].squeeze())\r\n",
        "    plt.scatter(np.arange(len(X_test1[k])), X_test1[k].squeeze(), cmap='inferno_r', c=c[k,:].squeeze(), s=50)\r\n",
        "    plt.plot(final_last_lstm[k], color = 'darkcyan')\r\n",
        "    plt.title('True label:' + str(N_test[k,-2]) + '   likelihood of label ' + str(N_test[k,-2]) + ': ' + str(softmax1_lstm[k][int(y_test[k])]))\r\n",
        "    # plt.title('True label:' + str(y_test[k]) + '   likelihood of label ' + str(y_test[k]) + ': ' + str(softmax1_lstm[k][int(y_test[k])]))\r\n",
        "    plt.clim(0.003,0.010)\r\n",
        "    plt.colorbar()\r\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKgc-YJODmsN"
      },
      "source": [
        "# # saving the model\r\n",
        "\r\n",
        "# model_json = lstmmodel.to_json()\r\n",
        "# with open(\"NEWlstmmodel_testunsam.json\", \"w\") as json_file:\r\n",
        "#     json_file.write(model_json)\r\n",
        "# # serialize weights to HDF5\r\n",
        "# lstmmodel.save_weights(\"NEWlstmmodel_testunsam_weights.h5\")\r\n",
        "# lstmmodel.save(\"NEWlstmmodel_testunsam.h5\")\r\n",
        "# print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2A9PhMSc_Vu"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n",
        "\r\n",
        "verbose, epoch, batch_size = 1, 5, 64\r\n",
        "activationFunction='relu'\r\n",
        "\r\n",
        "def getModel():\r\n",
        "    \r\n",
        "    cnnmodel = Sequential()\r\n",
        "    cnnmodel.add(Conv1D(filters=128, kernel_size=16,padding='same', activation='relu',input_shape=(X_train1.shape[1],1)))\r\n",
        "    cnnmodel.add(BatchNormalization())\r\n",
        "    cnnmodel.add(Conv1D(filters=32, kernel_size=16,padding='same', activation='relu'))\r\n",
        "    cnnmodel.add(BatchNormalization())\r\n",
        "    cnnmodel.add(Conv1D(filters=9, kernel_size=16,padding='same', activation='relu'))\r\n",
        "    cnnmodel.add(MaxPooling1D(pool_size=2,padding='same'))\r\n",
        "    cnnmodel.add(Flatten())\r\n",
        "    cnnmodel.add(Dense(512, activation='relu'))\r\n",
        "    cnnmodel.add(Dense(128, activation='relu'))\r\n",
        "    cnnmodel.add(Dense(32, activation='relu'))\r\n",
        "    cnnmodel.add(Dense(9, activation='softmax'))\r\n",
        "    cnnmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\r\n",
        "    cnnmodel.summary()\r\n",
        "    return cnnmodel\r\n",
        "\r\n",
        "cnnmodel = getModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_ozcehGpUQJ"
      },
      "source": [
        "modelhistory= cnnmodel.fit(X_train1[:,:,:,0], y_train1, epochs=epoch, verbose=verbose, validation_split=0.2, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2tAXFdKPiue"
      },
      "source": [
        "############## Get CAM ################\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# from matplotlib.backends.backend_pdf import PdfPages\r\n",
        "\r\n",
        "get_last_conv1 = keras.backend.function([cnnmodel.layers[0].input, keras.backend.learning_phase()], [cnnmodel.layers[4].output])\r\n",
        "last_conv1_cnn = get_last_conv1([X_test1[:1000,:,:,0]])[0]\r\n",
        "\r\n",
        "get_softmax1_cnn = keras.backend.function([cnnmodel.layers[0].input, keras.backend.learning_phase()], [cnnmodel.layers[-1].output])\r\n",
        "softmax1_cnn = get_softmax1_cnn(([X_test1[:1000,:,:,0]]))[0]\r\n",
        "softmax_weight1_cnn = cnnmodel.get_weights()[-1]\r\n",
        "softmax_weight_cnn = np.reshape(softmax_weight1_cnn,(9,1))\r\n",
        "\r\n",
        "CAM = np.dot(last_conv1_cnn, softmax_weight_cnn)\r\n",
        "\r\n",
        "# pp = PdfPages('CAM.pdf')\r\n",
        "for k in range(1):\r\n",
        "    CAM = (CAM - CAM.min(axis=1, keepdims=True)) / (CAM.max(axis=1, keepdims=True) - CAM.min(axis=1, keepdims=True))\r\n",
        "    c = np.exp(CAM) / np.sum(np.exp(CAM), axis=1, keepdims=True)\r\n",
        "    plt.figure(figsize=(18, 4))\r\n",
        "    plt.plot(X_test1[k].squeeze())\r\n",
        "    plt.scatter(np.arange(len(X_test1[k])), X_test1[k].squeeze(), cmap='inferno_r', c=c[k,:].squeeze(), s=50)\r\n",
        "    plt.title('True label:' + str(N_test[k,-2]) + '   likelihood of label ' + str(N_test[k,-2]) + ': ' + str(softmax1_cnn[k][int(y_test[k])]))\r\n",
        "    # plt.title('True label:' + str(y_test[k]) + '   likelihood of label ' + str(y_test[k]) + ': ' + str(softmax1_cnn[k][int(y_test[k])]))\r\n",
        "    plt.clim(0.003,0.010)\r\n",
        "    plt.colorbar()\r\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHy6YFLu1pvk"
      },
      "source": [
        "cnnpredictions = cnnmodel.predict(X_test1[:,:,:,0], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMTq2Za2EQVi"
      },
      "source": [
        "cnn_predict=np.argmax(cnnpredictions,axis=1)\r\n",
        "cnn_actual_value=np.argmax(y_test1,axis=1)\r\n",
        "CNN_cf_m = showResults(cnn_actual_value, cnn_predict)\r\n",
        "from sklearn import metrics\r\n",
        "metrics = metrics.classification_report(cnn_actual_value, cnn_predict, digits=3)\r\n",
        "print(metrics, 'CNN metrics')\r\n",
        "categories=['N','L','R','V','A','F','f','/']\r\n",
        "plt.figure(figsize=(8,6))\r\n",
        "CNN_cf_m = CNN_cf_m.astype('float')/ CNN_cf_m.sum(axis=1)[:,np.newaxis]\r\n",
        "sns.heatmap(CNN_cf_m,annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)\r\n",
        "plt.title('1D-CNN confusion matrix')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW9qw2RmjMp4"
      },
      "source": [
        "from itertools import islice\r\n",
        "def means_of_slices(iterable, slice_size):\r\n",
        "    iterator = iter(iterable)\r\n",
        "    while True:\r\n",
        "        slice = list(islice(iterator, slice_size))\r\n",
        "        if slice:\r\n",
        "            yield np.sum(slice)/len(slice)\r\n",
        "        else:\r\n",
        "            return\r\n",
        "a = last_conv1_cnn\r\n",
        "new_last_conv1 = []\r\n",
        "\r\n",
        "for i in range(len(last_conv1_cnn)):\r\n",
        "  means = list(means_of_slices(a[i], 20))\r\n",
        "  new_last_conv1.append(means)\r\n",
        "new_last_conv1 = np.array(new_last_conv1)\r\n",
        "print(new_last_conv1.shape)\r\n",
        "\r\n",
        "final_last_conv1 = []\r\n",
        "for i in new_last_conv1:\r\n",
        "  final_last_conv1.append(np.repeat(i,20))\r\n",
        "final_last_conv1 = np.array(final_last_conv1)\r\n",
        "print(final_last_conv1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckq6gRKVMu1r"
      },
      "source": [
        "import eli5\r\n",
        "from eli5 import format_as_image\r\n",
        "from eli5.sklearn import PermutationImportance\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "NNMLP_clf = MLPClassifier(random_state=48, max_iter=50)\r\n",
        "NNMLP_clf.fit(new_last_conv1, y_test1[:1000])\r\n",
        "perm = PermutationImportance(NNMLP_clf).fit(new_last_conv1, y_test1[:1000])\r\n",
        "print('CNN results')\r\n",
        "eli5.show_weights(perm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY9UPaMoqnKY"
      },
      "source": [
        "conv_df_slices = pd.DataFrame(new_last_conv1)\r\n",
        "corr_slices = conv_df_slices.corr(method='kendall')\r\n",
        "slices_nums = [1,2,3,4,5,6,7,8,9,10,11]\r\n",
        "rcParams['figure.figsize'] = 8,6\r\n",
        "sns.heatmap(corr_slices,annot=True,xticklabels=slices_nums,yticklabels=slices_nums,vmin=-0.6,vmax=1,fmt='0.1g')\r\n",
        "plt.title('CNN Kendall Tau Corellation')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHzYvg4a0RpC"
      },
      "source": [
        "############## Get CAM ################\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "CAM = np.dot(last_conv1_cnn, softmax_weight_cnn)\r\n",
        "\r\n",
        "# pp = PdfPages('CAM.pdf')\r\n",
        "for k in range(1):\r\n",
        "    CAM = (CAM - CAM.min(axis=1, keepdims=True)) / (CAM.max(axis=1, keepdims=True) - CAM.min(axis=1, keepdims=True))\r\n",
        "    c = np.exp(CAM) / np.sum(np.exp(CAM), axis=1, keepdims=True)\r\n",
        "    plt.figure(figsize=(18, 4))\r\n",
        "    plt.plot(X_test1[k].squeeze())\r\n",
        "    plt.scatter(np.arange(len(X_test1[k])), X_test1[k].squeeze(), cmap='inferno_r', c=c[k,:].squeeze(), s=50)\r\n",
        "    plt.plot(final_last_conv1[k], color = 'darkcyan')\r\n",
        "    plt.title('True label:' + str(N_test[k,-2]) + '   likelihood of label ' + str(N_test[k,-2]) + ': ' + str(softmax1_cnn[k][int(y_test[k])]))\r\n",
        "    # plt.title('True label:' + str(y_test[k]) + '   likelihood of label ' + str(y_test[k]) + ': ' + str(softmax1_cnn[k][int(y_test[k])]))\r\n",
        "    plt.clim(0.003,0.010)\r\n",
        "    plt.colorbar()\r\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lit3twTU0iQk"
      },
      "source": [
        "from sklearn.preprocessing import normalize\r\n",
        "\r\n",
        "n1= normalize(final_last_conv1)\r\n",
        "n2= normalize(final_last_lstm)\r\n",
        "n3= normalize(last_conv1_cnn[:,:,0])\r\n",
        "n4= normalize(last_conv1_lstm[:,:,0])\r\n",
        "n5= normalize(X_test)\r\n",
        "\r\n",
        "m1=n1.mean(axis=0)\r\n",
        "m2=n2.mean(axis=0)\r\n",
        "m3=n3.mean(axis=0)\r\n",
        "m4=n4.mean(axis=0)\r\n",
        "\r\n",
        "\r\n",
        "plt.figure(figsize=(12,4))\r\n",
        "plt.plot(n5[0],color='b')\r\n",
        "plt.plot(n5[0],color='b',label='ECG Beat')\r\n",
        "plt.plot(m1,color='g',label='Conv1D layer')\r\n",
        "plt.plot(m2,color='r',label='LSTM layer')\r\n",
        "plt.title('Class: N, Number: 1')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.figure(figsize=(12,4))\r\n",
        "plt.plot(n5[0],color='b')\r\n",
        "plt.plot(n5[0],color='b',label='ECG Beat')\r\n",
        "plt.plot(m1*2,color='g',label='Conv1D layer')\r\n",
        "plt.plot(m2*2,color='r',label='LSTM layer')\r\n",
        "plt.title('Class: N, Number: 1')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H85NZq3v49ur"
      },
      "source": [
        "# # saving the model\n",
        "\n",
        "# model_json = cnnmodel.to_json()\n",
        "# with open(\"NEWcnnmodel_testunsam.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# # serialize weights to HDF5\n",
        "# cnnmodel.save_weights(\"NEWcnnmodel_testunsam_weights.h5\")\n",
        "# cnnmodel.save(\"NEWcnnmodel_testunsam.h5\")\n",
        "# print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTnZ8bAEN2j"
      },
      "source": [
        "# from sklearn.decomposition import PCA\r\n",
        "# x_pca = PCA(n_components=50,random_state=42).fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lla-05ZEEUjj"
      },
      "source": [
        "# plt.figure(figsize=(8,8))\r\n",
        "\r\n",
        "# categories=['N','L','R','V','A','F','f','/']\r\n",
        "# scatter = plt.scatter(x_pca[:,0],x_pca[:,1],c=y_train, cmap='inferno')\r\n",
        "# plt.legend(title=\"Classes\",loc='upper right',*scatter.legend_elements())\r\n",
        "# plt.title('PCA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLSbCNbBX9VS"
      },
      "source": [
        "# from sklearn.manifold import TSNE\r\n",
        "# x_tsne = TSNE(n_components=2,random_state=42,perplexity=100, verbose=5).fit_transform(x_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziopHCkiY_E3"
      },
      "source": [
        "# plt.figure(figsize=(8,8))\r\n",
        "\r\n",
        "# categories=['N','L','R','V','A','F','f','/']\r\n",
        "# scatter = plt.scatter(x_tsne[:,0],x_tsne[:,1],c=y_train, cmap='inferno')\r\n",
        "# plt.legend(title=\"Classes\",loc='upper right',*scatter.legend_elements())\r\n",
        "# plt.title('PCA and TSNE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B8JAS1R9o3G"
      },
      "source": [
        "# import umap\r\n",
        "# reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=100, a = 0.5, b = 1.2)\r\n",
        "# embedding = reducer.fit_transform(x_pca)\r\n",
        "# embedding.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycGum_a_-CZz"
      },
      "source": [
        "# plt.figure(figsize=(8,8))\r\n",
        "\r\n",
        "# plt.scatter(reducer.embedding_[:, 0], reducer.embedding_[:, 1], c=y_train, cmap='inferno')\r\n",
        "# plt.legend(title=\"Classes\",loc='upper right',*scatter.legend_elements())\r\n",
        "# plt.title('PCA and UMAP')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtRid33iJ7Aq"
      },
      "source": [
        "# from keras.models import model_from_json\n",
        "# #loading the model and running it on datasets\n",
        "# from keras.models import load_model\n",
        "\n",
        "# saved_model = load_model('/content/drive/My Drive/compsci/modelsresult/NEWcnnmodel_testsam.h5')\n",
        "# saved_model.summary()\n",
        "# json_file = open('/content/drive/My Drive/compsci/modelsresult/NEWcnnmodel_testsam.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "# # load weights into new model\n",
        "# loaded_model.load_weights(\"/content/drive/My Drive/compsci/modelsresult/NEWcnnmodel_testsam_weights.h5\")\n",
        "# print(\"Loaded model from disk\")\n",
        "\n",
        "# loaded_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# score = loaded_model.evaluate(X_test1[:,:,:,0], y_test1, verbose=1)\n",
        "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a2QIkIApiXQ"
      },
      "source": [
        " # # Plot training & validation loss values\n",
        "# plt.plot(modelhistory.history['loss'])\n",
        "# plt.plot(modelhistory.history['val_loss'])\n",
        "# plt.title('Model loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "# plt.savefig('Loss.png', format='png', dpi=1200)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # Plot training & validation accuracy values\n",
        "# plt.plot(modelhistory.history['accuracy'])\n",
        "# plt.plot(modelhistory.history['val_accuracy'])\n",
        "# plt.title('Model accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "# plt.savefig('Accuracy.png', format='png', dpi=1200)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYLY9Th-bKTY"
      },
      "source": [
        "# from sklearn import metrics\r\n",
        "# y_pred_CNN = loaded_model.predict(X_test1[:,:,:,0])\r\n",
        "# y_pred_CNN = np.argmax(y_pred_CNN, axis=1)\r\n",
        "# y_test = np.argmax(y_test1, axis=1)\r\n",
        "# print(y_pred_CNN)\r\n",
        "# metrics = metrics.classification_report(y_test, y_pred_CNN, digits=3)\r\n",
        "# print(metrics, 'CNN metrics')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8IVw5OwGArT"
      },
      "source": [
        "# CNN_cf_m = showResults(y_test, y_pred_CNN)\r\n",
        "# from sklearn import metrics\r\n",
        "# metrics = metrics.classification_report(y_test, y_pred_CNN, digits=3)\r\n",
        "# print(metrics, 'CNN metrics')\r\n",
        "# categories=['N','L','R','V','A','F','f','/']\r\n",
        "# plt.figure(figsize=(8,6))\r\n",
        "# CNN_cf_m = CNN_cf_m.astype('float')/ CNN_cf_m.sum(axis=1)[:,np.newaxis]\r\n",
        "# sns.heatmap(CNN_cf_m,annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)\r\n",
        "# plt.title('1D-CNN confusion matrix')\r\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCeuvU-2s8V"
      },
      "source": [
        "# N_train = train_values[train_values[:,-2]==7]\r\n",
        "# train_df = pd.DataFrame(N_train[:,:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rYDvzawxhZF"
      },
      "source": [
        "# for i in range(20):\r\n",
        "#   sns.set()\r\n",
        "#   plt.figure(figsize=(6,2))\r\n",
        "#   plt.plot(train_df.iloc[i,:186])\r\n",
        "#   label = train_df.iloc[i,187]\r\n",
        "#   print(label)\r\n",
        "#   plt.title('Class: f, Number: 7')\r\n",
        "#   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}