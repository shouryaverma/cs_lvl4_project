{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import glob\n",
    "from wfdb import processing\n",
    "import scipy\n",
    "from scipy import *\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob.glob('C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih/*.atr')\n",
    "data_files = [i[:-4] for i in data_files]\n",
    "data_files.sort()\n",
    "print(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['N','R','L','V','A','E']\n",
    "record_nums =[]\n",
    "record_names = []\n",
    "num_beats =[]\n",
    "\n",
    "for i in range(len(data_files)):\n",
    "    signals, fields = wfdb.rdsamp(data_files[i])\n",
    "    annotation = wfdb.rdann(data_files[i], 'atr')\n",
    "    record_nums.append(i)\n",
    "#   record_names.append(data_files[i])\n",
    "    for j in classes:\n",
    "        ids = np.in1d(annotation.symbol, j)\n",
    "        beats = annotation.sample[ids]\n",
    "        num_beats.append(len(beats))\n",
    "        \n",
    "n_vals = num_beats[::6]\n",
    "r_vals = num_beats[1::6]\n",
    "l_vals = num_beats[2::6]\n",
    "v_vals = num_beats[3::6]\n",
    "a_vals = num_beats[4::6]\n",
    "e_vals = num_beats[5::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb as wf\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from biosppy.signals import ecg\n",
    "from scipy import signal\n",
    "\n",
    "def extract_data():\n",
    "    data_files = glob.glob('C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih/*.atr')\n",
    "    data_files = [i[:-4] for i in data_files]\n",
    "    data_files.sort()\n",
    "    return data_files\n",
    "\n",
    "files = extract_data()\n",
    "i=0\n",
    "\n",
    "good_beats =['N','L','R','B','A','a','J','S','V','r',\n",
    "             'F','e','j','n','E','/','f','Q','?']\n",
    "list_anns = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    datfile = files[i]\n",
    "    record = wf.rdsamp(datfile)\n",
    "    ann = wf.rdann(datfile, 'atr')\n",
    "    list_anns.extend(ann.symbol)\n",
    "\n",
    "dict_anns = {}\n",
    "\n",
    "for i in list_anns:\n",
    "    dict_anns[i] = dict_anns.get(i,0)+1\n",
    "    \n",
    "dict_anns = {k:v for k,v in dict_anns.items() if k in good_beats}\n",
    "\n",
    "print(dict_anns.values())\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "xlocs, xlabs = plt.xticks()\n",
    "barplt = plt.bar(list(dict_anns.keys()),dict_anns.values(),width = .6)\n",
    "xlocs = [j for j in dict_anns.keys()]\n",
    "ylabs = [j for j in dict_anns.values()]\n",
    "\n",
    "plt.title('Annotation Distribution for all Classes in the Dataset')\n",
    "plt.xlabel('Annotations')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "for bar in barplt:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x(), yval+1000, yval)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wfdb as wf\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from biosppy.signals import ecg\n",
    "from scipy import signal\n",
    "\n",
    "def extract_data():\n",
    "    data_files = glob.glob('C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih/*.atr')\n",
    "    data_files = [i[:-4] for i in data_files]\n",
    "    data_files.sort()\n",
    "    return data_files\n",
    "\n",
    "records = extract_data()\n",
    "print('Total files: ', len(records))\n",
    "\n",
    "# Instead of using the annotations to find the beats, we will\n",
    "# use R-peak detection instead. The reason for this is so that\n",
    "# the same logic can be used to analyze new and un-annotated\n",
    "# ECG data. We use the annotations here only to classify the\n",
    "# beat as either Normal or Abnormal and to train the model.\n",
    "# Reference:\n",
    "# https://physionet.org/physiobank/database/html/mitdbdir/intro.htm\n",
    "\n",
    "good_beats = ['N','L','R','B','A','a','J','S','V','r',\n",
    "             'F','e','j','n','E','/','f','Q','?']\n",
    "\n",
    "# Loop through each input file. Each file contains one\n",
    "# record of ECG readings, sampled at 360 readings per\n",
    "# second.\n",
    "\n",
    "\n",
    "for path in records:\n",
    "    pathpts = path.split('/')\n",
    "    fn = pathpts[-1]\n",
    "    print('Loading file:', path)\n",
    "\n",
    "    # Read in the data\n",
    "    record = wf.rdsamp(path)\n",
    "    annotation = wf.rdann(path, 'atr')\n",
    "\n",
    "    # Print some meta informations\n",
    "    print('    Sampling frequency used for this record:', record[1].get('fs'))\n",
    "    print('    Shape of loaded data array:', record[0].shape)\n",
    "    print('    Number of loaded annotations:', len(annotation.num))\n",
    "    \n",
    "    # Get the ECG values from the file.\n",
    "    data = record[0].transpose()\n",
    "\n",
    "    # Generate the classifications based on the annotations.\n",
    "    # 0.0 = undetermined\n",
    "    # 1.0 = normal\n",
    "    # 2.0 = LBBBB\n",
    "    # 3.0 = RBBBB\n",
    "    # 4.0 = Premature Ventricular contraction\n",
    "    # 5.0 = Atrial Premature beat\n",
    "    # 6.0 = Fusion ventricular normal beat\n",
    "    # 7.0 = Fusion of paced and normal beat\n",
    "    # 8.0 = paced beat\n",
    "    \n",
    "    clas = np.array(annotation.symbol)\n",
    "    rate = np.zeros_like(clas, dtype='float')\n",
    "    for clasid, clasval in enumerate(clas):\n",
    "        if (clasval == 'N'):\n",
    "            rate[clasid] = 1.0 # Normal\n",
    "        elif (clasval == 'L'):\n",
    "            rate[clasid] = 2.0 # LBBBB\n",
    "        elif (clasval == 'R'):\n",
    "            rate[clasid] = 3.0 # RBBBB\n",
    "        elif (clasval == 'V'):\n",
    "            rate[clasid] = 4.0 # Premature Ventricular contraction\n",
    "        elif (clasval == 'A'):\n",
    "            rate[clasid] = 5.0 # Atrial Premature beat\n",
    "        elif (clasval == 'F'):\n",
    "            rate[clasid] = 6.0 # Fusion ventricular normal beat\n",
    "        elif (clasval == 'f'):\n",
    "            rate[clasid] = 7.0 # Fusion of paced and normal beat\n",
    "        elif (clasval == '/'):\n",
    "            rate[clasid] = 8.0 # paced beat\n",
    "            \n",
    "    rates = np.zeros_like(data[0], dtype='float')\n",
    "    rates[annotation.sample] = rate\n",
    "    \n",
    "    indices = np.arange(data[0].size, dtype='int')\n",
    "\n",
    "    # Manipulate both channels\n",
    "    for channelid, channel in enumerate(data):\n",
    "        chname = record[1].get('sig_name')[channelid]\n",
    "        print('    ECG channel type:', chname)\n",
    "        \n",
    "        # Find rpeaks in the ECG data. Most should match with\n",
    "        # the annotations.\n",
    "        out = ecg.ecg(signal=channel, sampling_rate=360, show=False)\n",
    "\n",
    "        # Split into individual heartbeats. For each heartbeat\n",
    "        # record, append classification.\n",
    "        \n",
    "        beats = []\n",
    "        for ind, ind_val in enumerate(out['rpeaks']):\n",
    "\n",
    "            start,end = ind_val-220//2, ind_val+220//2\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "            diff = 220 - len(channel[start:end])\n",
    "            if diff > 0:\n",
    "                padding = np.zeros(diff, dtype='float')\n",
    "                padded_channel = np.append(padding, channel[start:end])\n",
    "                beats.append(padded_channel)\n",
    "            else:\n",
    "                beats.append(channel[start:end])\n",
    "\n",
    "            # Get the classification value that is on\n",
    "            # or near the position of the rpeak index.\n",
    "            from_ind = 0 if ind_val < 10 else ind_val - 10\n",
    "            to_ind = ind_val + 10\n",
    "            clasval = rates[from_ind:to_ind].max()\n",
    "            \n",
    "\n",
    "            # Normalize the data\n",
    "            #beats[ind] = (beats[ind] - beats[ind].min()) / (beats[ind].max() - beats[ind].min())\n",
    "            \n",
    "            # Standardize the data\n",
    "            beats[ind] = ((beats[ind] - np.mean(beats[ind])) / np.std(beats[ind]))\n",
    "\n",
    "            # Append the classification to the beat data.\n",
    "            beats[ind] = np.append(beats[ind], clasval)\n",
    "            \n",
    "            # Append the record number to the beat data.\n",
    "            beats[ind] = np.append(beats[ind], fn[-3:])\n",
    "\n",
    "        # Save to CSV file.\n",
    "\n",
    "        savedata = np.array(beats[:], dtype=np.float)\n",
    "        outfn = 'C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/'+fn+'_'+chname+'.csv'\n",
    "        print('    Generating ', outfn)\n",
    "        with open(outfn, \"wb\") as fin:\n",
    "            np.savetxt(fin, savedata, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_data = np.loadtxt('C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih/123_MLII.csv', delimiter=',')\n",
    "\n",
    "beat_num = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "for beatid in beat_num:\n",
    "    times = np.arange(200, dtype = 'float') / 200\n",
    "    beat = csv_data[beatid][:-2]\n",
    "    anno = csv_data[beatid][-2]\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    if (anno == 1.0):\n",
    "        plt.plot(times, beat, 'b')\n",
    "        plt.ylabel('ann = N', size=20)\n",
    "    elif(anno == 2.0):\n",
    "        plt.plot(times, beat, 'r')\n",
    "        plt.ylabel('ann = L', size=20)\n",
    "    elif(anno == 3.0):\n",
    "        plt.plot(times, beat, 'g')\n",
    "        plt.ylabel('ann = R', size=20)\n",
    "    elif(anno == 4.0):\n",
    "        plt.plot(times, beat, 'y')\n",
    "        plt.ylabel('ann = V', size=20)\n",
    "    elif(anno == 5.0):\n",
    "        plt.plot(times, beat, 'c')\n",
    "        plt.ylabel('ann = A', size=20)\n",
    "    elif(anno == 6.0):\n",
    "        plt.plot(times, beat, 'm')\n",
    "        plt.ylabel('ann = F', size=20)\n",
    "    elif(anno == 7.0):\n",
    "        plt.plot(times, beat, 'k')\n",
    "        plt.ylabel('ann = f', size=20)\n",
    "    elif(anno == 8.0):\n",
    "        plt.plot(times, beat, 'm')\n",
    "        plt.ylabel('ann = /', size=20)\n",
    "        \n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.title(' | beat ' + str(beatid) + \" type \" + str(anno)+' | ')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the classifications based on the annotations.\n",
    "\n",
    "* 0.0 = undetermined\n",
    "* N = 1.0 = normal\n",
    "* L = 2.0 = LBBBB\n",
    "* R = 3.0 = RBBBB\n",
    "* V = 4.0 = Premature Ventricular contraction\n",
    "* A = 5.0 = Atrial Premature beat\n",
    "* F = 6.0 = Fusion ventricular normal beat\n",
    "* f = 7.0 = Fusion of paced and normal beat\n",
    "* / = 8.0 = paced beat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.frontiersin.org/articles/10.3389/fphy.2019.00103/full\n",
    "\n",
    "https://arxiv.org/pdf/2007.02052.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 222)\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\100_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\100_V5.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\101_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\101_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\102_V2.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\102_V5.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\103_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\103_V2.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\104_V2.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\104_V5.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\105_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\105_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\106_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\106_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\107_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\107_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\108_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\108_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\109_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\109_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\111_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\111_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\112_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\112_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\113_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\113_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\114_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\114_V5.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\115_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\115_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\116_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\116_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\117_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\117_V2.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\118_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\118_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\119_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\119_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\121_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\121_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\122_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\122_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\123_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\123_V5.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\124_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\124_V4.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\200_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\200_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\201_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\201_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\202_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\202_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\203_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\203_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\205_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\205_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\207_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\207_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\208_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\208_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\209_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\209_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\210_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\210_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\212_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\212_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\213_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\213_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\214_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\214_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\215_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\215_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\217_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\217_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\219_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\219_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\220_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\220_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\221_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\221_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\222_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\222_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\223_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\223_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\228_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\228_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\230_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\230_V1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\231_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\231_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\232_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\232_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\233_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\233_V1.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\234_MLII.csv\n",
      "Loading  C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih\\234_V1.csv\n",
      "(215361, 222)\n"
     ]
    }
   ],
   "source": [
    "alldata = np.empty(shape=[0, 222])\n",
    "print(alldata.shape)\n",
    "all_csvs = glob.glob('C:/Users/shour/Desktop/Compsci project workspace/cs_lvl4_project/mit-bih/*.csv')\n",
    "for j in all_csvs:\n",
    "    print('Loading ', j)\n",
    "    csvrows = np.loadtxt(j, delimiter=',')\n",
    "    alldata = np.append(alldata, csvrows, axis=0)\n",
    "    \n",
    "print(alldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63909,)\n",
      "(151452, 222)\n"
     ]
    }
   ],
   "source": [
    "no_anno = np.where(alldata[:,-2]==0.0)[0]\n",
    "print(no_anno.shape)\n",
    "alldata = np.delete(alldata, no_anno,0)\n",
    "print(alldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151452, 222)\n",
      "(113589, 222)\n",
      "(37863, 222)\n"
     ]
    }
   ],
   "source": [
    "print(alldata.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(alldata,test_size=0.25,random_state=48)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "N = train[train[:,-2]==1.0]\n",
    "L = train[train[:,-2]==2.0]\n",
    "R = train[train[:,-2]==3.0]\n",
    "V = train[train[:,-2]==4.0]\n",
    "A = train[train[:,-2]==5.0]\n",
    "F = train[train[:,-2]==6.0]\n",
    "f = train[train[:,-2]==7.0]\n",
    "I = train[train[:,-2]==8.0]\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "def downsample(arr, n, seed):\n",
    "    downsampled = resample(arr,replace=False,n_samples=n, random_state=seed)\n",
    "    return downsampled\n",
    "\n",
    "def upsample(arr, n, seed):\n",
    "    upsampled = resample(arr,replace=True,n_samples=n,random_state=seed)\n",
    "    return upsampled\n",
    "\n",
    "all_class = [N,L,R,V,A,F,f,I]\n",
    "abn_class = [L,R,V,A,F,f,I]\n",
    "\n",
    "mean_val = np.mean([len(i) for i in abn_class], dtype= int)\n",
    "train_sampled = []\n",
    "\n",
    "for i in all_class:\n",
    "    if i.shape[0]> mean_val:\n",
    "        i = downsample(i,mean_val,seed)\n",
    "    elif i.shape[0]< mean_val:\n",
    "        i = upsample(i, mean_val,seed)\n",
    "    train_sampled.append(i)\n",
    "    \n",
    "train_sampled = np.concatenate(train_sampled)\n",
    "np.random.shuffle(train_sampled)\n",
    "train_sampled_all = train_sampled\n",
    "\n",
    "with open('train_sampled_all_220.csv', \"wb\") as fin:\n",
    "    np.savetxt(fin, train_sampled_all, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = test[test[:,-2]==1.0]\n",
    "L = test[test[:,-2]==2.0]\n",
    "R = test[test[:,-2]==3.0]\n",
    "V = test[test[:,-2]==4.0]\n",
    "A = test[test[:,-2]==5.0]\n",
    "F = test[test[:,-2]==6.0]\n",
    "f = test[test[:,-2]==7.0]\n",
    "I = test[test[:,-2]==8.0]\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "def downsample(arr, n, seed):\n",
    "    downsampled = resample(arr,replace=False,n_samples=n, random_state=seed)\n",
    "    return downsampled\n",
    "\n",
    "def upsample(arr, n, seed):\n",
    "    upsampled = resample(arr,replace=True,n_samples=n,random_state=seed)\n",
    "    return upsampled\n",
    "\n",
    "all_class = [N,L,R,V,A,F,f,I]\n",
    "abn_class = [L,R,V,A,F,f,I]\n",
    "\n",
    "mean_val = np.mean([len(i) for i in abn_class], dtype= int)\n",
    "test_sampled = []\n",
    "\n",
    "for i in all_class:\n",
    "    if i.shape[0]> mean_val:\n",
    "        i = downsample(i,mean_val,seed)\n",
    "    elif i.shape[0]< mean_val:\n",
    "        i = upsample(i, mean_val,seed)\n",
    "    test_sampled.append(i)\n",
    "    \n",
    "test_sampled = np.concatenate(test_sampled)\n",
    "np.random.shuffle(test_sampled)\n",
    "test_sampled_all = test_sampled\n",
    "    \n",
    "with open('test_sampled_all_220.csv', \"wb\") as fin:\n",
    "    np.savetxt(fin, test_sampled_all, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_unsampled_all_260.csv', \"wb\") as fin:\n",
    "    np.savetxt(fin, test, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['N','L','R','V','A','F','f','/']\n",
    "\n",
    "dftrain = pd.DataFrame(train)\n",
    "countstrain = dftrain[187].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "barplt = plt.bar(countstrain.index, countstrain.values, alpha=0.8, color='orange')\n",
    "plt.title('Training Unsampled Label Count')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Beat Category', fontsize=12)\n",
    "plt.xticks(ticks=[1,2,3,4,5,6,7,8],labels=feature_names)\n",
    "for bar in barplt:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x()+.2, yval+1000, yval)\n",
    "plt.show()\n",
    "\n",
    "df_sampled = pd.DataFrame(train_sampled_all)\n",
    "counts = df_sampled[187].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "barplt = plt.bar(counts.index, counts.values, alpha=0.8, color='b')\n",
    "plt.title('Training Sampled Label Count')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Beat Category', fontsize=12)\n",
    "plt.xticks(ticks=[1,2,3,4,5,6,7,8],labels=feature_names)\n",
    "for bar in barplt:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x()+.2, yval+30, yval)\n",
    "plt.show()\n",
    "\n",
    "dftest = pd.DataFrame(test)\n",
    "countstest = dftest[187].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "barplt = plt.bar(countstest.index, countstest.values, alpha=0.8, color='r')\n",
    "plt.title('Testing Unsampled Label Count')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Beat Category', fontsize=12)\n",
    "plt.xticks(ticks=[1,2,3,4,5,6,7,8],labels=feature_names)\n",
    "for bar in barplt:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x()+.2, yval+600, yval)\n",
    "plt.show()\n",
    "\n",
    "dftest_sampled = pd.DataFrame(test_sampled_all)\n",
    "countstestsam = dftest_sampled[187].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "barplt = plt.bar(countstestsam.index, countstestsam.values, alpha=0.8, color='g')\n",
    "plt.title('Testing Sampled Label Count')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Beat Category', fontsize=12)\n",
    "plt.xticks(ticks=[1,2,3,4,5,6,7,8],labels=feature_names)\n",
    "for bar in barplt:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x()+.2, yval+20, yval)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import glob\n",
    "\n",
    "# train_values = np.empty(shape=[0, 189])\n",
    "# test_values = np.empty(shape=[0, 189]) \n",
    "# train_sampled_all = glob.glob('./train_sampled_all.csv')\n",
    "# test_all = glob.glob('./test_all.csv')\n",
    "\n",
    "# for j in train_sampled_all:\n",
    "#     print('Loading ', j)\n",
    "#     csvrows = np.loadtxt(j, delimiter=',')\n",
    "#     train_values = np.append(train_values, csvrows, axis=0)\n",
    "    \n",
    "# for j in test_all:\n",
    "#     print('Loading ', j)\n",
    "#     csvrows = np.loadtxt(j, delimiter=',')\n",
    "#     test_values = np.append(test_values, csvrows, axis=0)\n",
    "    \n",
    "# print(train_values.shape)\n",
    "# print(test_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_values[:,:-2]\n",
    "# X_test = test_values[:,:-2]\n",
    "\n",
    "# y_train = train_values[:,-2]\n",
    "# y_test = test_values[:,-2]\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# # using gradient boost classifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# gbc_clf = GradientBoostingClassifier(n_estimators=10,random_state=48)\n",
    "# gbc_clf.fit(X_train, y_train)\n",
    "# print('Gradient Boosting Results')\n",
    "# y_pred_gbc = gbc_clf.predict(X_test)\n",
    "# print(gbc_clf.score(X_test,y_test))\n",
    "\n",
    "# # using adaboost classifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# ada_clf = AdaBoostClassifier(n_estimators=10,random_state=48)\n",
    "# ada_clf.fit(X_train, y_train)\n",
    "# print('Ada Boosting Results')\n",
    "# y_pred_ada = ada_clf.predict(X_test)\n",
    "# print(ada_clf.score(X_test,y_test))\n",
    "\n",
    "# # using random forest classifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rfc_clf = RandomForestClassifier(max_depth=10, random_state=48,n_estimators=10)\n",
    "# rfc_clf.fit(X_train, y_train)\n",
    "# print('Random Forest Results')\n",
    "# y_pred_rfc = rfc_clf.predict(X_test)\n",
    "# print(rfc_clf.score(X_test, y_test))\n",
    "\n",
    "# # using naive bayes\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# NB_clf = GaussianNB()\n",
    "# NB_clf.fit(X_train, y_train)\n",
    "# print('Naive Bayes Results')\n",
    "# y_pred_NB = NB_clf.predict(X_test)\n",
    "# print(NB_clf.score(X_test, y_test))\n",
    "\n",
    "# # using NN Multi Layer Perceptron classifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# NNMLP_clf = MLPClassifier(random_state=48, max_iter=50)\n",
    "# NNMLP_clf.fit(X_train, y_train)\n",
    "# print('NNMLP Results')\n",
    "# y_pred_NNMLP = NNMLP_clf.predict(X_test)\n",
    "# print(NNMLP_clf.score(X_test, y_test))\n",
    "\n",
    "# #support vector classifier\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# svc_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "# svc_clf.fit(X_train, y_train)\n",
    "# print('Support Vector Results')\n",
    "# y_pred_svc = svc_clf.predict(X_test)\n",
    "# print(svc_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from sklearn.metrics import *\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# gbc_cf_m = confusion_matrix(y_test,y_pred_gbc)\n",
    "# gbc_cf_m = gbc_cf_m.astype('float')/ gbc_cf_m.sum(axis=1)[:,np.newaxis]\n",
    "# gbc_pr_s = precision_score(y_test,y_pred_gbc,average='weighted')\n",
    "\n",
    "# ada_cf_m = confusion_matrix(y_test,y_pred_ada)\n",
    "# ada_pr_s = precision_score(y_test,y_pred_ada,average='weighted')\n",
    "\n",
    "# rfc_cf_m = confusion_matrix(y_test,y_pred_rfc)\n",
    "# rfc_pr_s = precision_score(y_test,y_pred_rfc,average='weighted')\n",
    "\n",
    "# NB_cf_m = confusion_matrix(y_test,y_pred_NB)\n",
    "# NB_pr_s = precision_score(y_test,y_pred_NB,average='weighted')\n",
    "\n",
    "# NNMLP_cf_m = confusion_matrix(y_test,y_pred_NNMLP)\n",
    "# NNMLP_pr_s = precision_score(y_test,y_pred_NNMLP,average='weighted')\n",
    "\n",
    "# svc_cf_m = confusion_matrix(y_test,y_pred_svc)\n",
    "# svc_pr_s = precision_score(y_test,y_pred_svc,average='weighted')\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(gbc_cf_m,annot=True,fmt='.2%')\n",
    "# plt.title('GBC confusion matrix')\n",
    "# plt.savefig('gbc_cfm.jpeg')\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(ada_cf_m/np.sum(ada_cf_m),annot=True,fmt='.2%')\n",
    "# plt.title('ADA confusion matrix')\n",
    "# plt.savefig('ada_cfm.jpeg')\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(rfc_cf_m/np.sum(rfc_cf_m),annot=True,fmt='.2%')\n",
    "# plt.title('RFC confusion matrix')\n",
    "# plt.savefig('rfc_cfm.jpeg')\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(NB_cf_m/np.sum(NB_cf_m),annot=True,fmt='.2%')\n",
    "# plt.title('NB confusion matrix')\n",
    "# plt.savefig('NB_cfm.jpeg')\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(NNMLP_cf_m/np.sum(NNMLP_cf_m),annot=True,fmt='.2%')\n",
    "# plt.title('NNMLP confusion matrix')\n",
    "# plt.savefig('NNMLP_cfm.jpeg')\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(svc_cf_m/np.sum(svc_cf_m),annot=True,fmt='.2%')\n",
    "# plt.title('SVC confusion matrix')\n",
    "# plt.savefig('SVC_cfm.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "# xfit = clf.fit(X_train, y_train)\n",
    "# clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "x_pca = PCA(n_components=50,random_state=42).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tsne = TSNE(n_components=3,random_state=42,perplexity=50).fit_transform(x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_tsne[:,0],x_tsne[:,1], c=y_train)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\",loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.figure(figsize=(16,16)).gca(projection='3d')\n",
    "# ax.scatter(\n",
    "#     xs=x_tsne[:,0],\n",
    "#     ys=x_tsne[:,1],\n",
    "#     zs=x_tsne[:,2],\n",
    "#     c=y_train,\n",
    "#     cmap='tab10'\n",
    "# )\n",
    "# ax.set_xlabel('pca-one')\n",
    "# ax.set_ylabel('pca-two')\n",
    "# ax.set_zlabel('pca-three')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import umap\n",
    "\n",
    "# x_pca = PCA(n_components=2,random_state=42).fit_transform(X_train)\n",
    "# x_tsne = TSNE(n_components=2,random_state=42,perplexity=50).fit_transform(x_pca)\n",
    "# x_umap = umap.UMAP().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# scatter = plt.scatter(x_tsne[:,0],x_tsne[:,1], c=y_train)\n",
    "# plt.legend(*scatter.legend_elements(), title=\"Classes\",loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# def leave_one_patient():\n",
    "#     x = values[:,:-2]\n",
    "#     y = values[:,-2]\n",
    "#     groups = values[:,-1]\n",
    "#     lopo = LeaveOneGroupOut()\n",
    "#     lopo.get_n_splits(x,y,groups)\n",
    "#     x_train = []\n",
    "#     x_test = []\n",
    "#     y_train = []\n",
    "#     y_test = []\n",
    "#     for train_index, test_index in lopo.split(x,y,groups):\n",
    "#         x_tr, x_te = x[train_index], x[test_index]\n",
    "#         y_tr, y_te = y[train_index], y[test_index]\n",
    "#         x_train.append(x_tr)\n",
    "#         x_test.append(x_te)\n",
    "#         y_train.append(y_tr)\n",
    "#         y_test.append(y_te)\n",
    "\n",
    "#     return x_train, x_test, y_train, y_test\n",
    "\n",
    "# x_train, x_test, y_train, y_test = leave_one_patient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
