{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "correlation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw8Sc5HYgy7f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "from scipy.stats.stats import kendalltau\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import log_loss\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import LSTM, Masking\n",
        "from tensorflow.keras.layers import MaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras import activations\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eB1mZ_gNpKj"
      },
      "source": [
        "# #permutation feature importance values\n",
        "# gbc_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/gbc_pfi.csv')\n",
        "# ada_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/ada_pfi.csv')\n",
        "# rfc_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/rfc_pfi.csv')\n",
        "# nb_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/nb_pfi.csv')\n",
        "# nnmlp_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/nnmlp_pfi.csv')\n",
        "# svc_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/svc_pfi.csv')\n",
        "# lstm_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/lstm_pfi.csv')\n",
        "# cnn_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/cnn_pfi.csv')\n",
        "\n",
        "# gbc_pfi = gbc_pfi.weight.values\n",
        "# ada_pfi = ada_pfi.weight.values\n",
        "# rfc_pfi = rfc_pfi.weight.values\n",
        "# nb_pfi = nb_pfi.weight.values\n",
        "# nnmlp_pfi = nnmlp_pfi.weight.values\n",
        "# svc_pfi = svc_pfi.weight.values\n",
        "# lstm_pfi = lstm_pfi.weight.values\n",
        "# cnn_pfi = cnn_pfi.weight.values\n",
        "\n",
        "# gbc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/gbc_pfi_corr.csv')\n",
        "# ada_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/ada_pfi_corr.csv')\n",
        "# rfc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/rfc_pfi_corr.csv')\n",
        "# nb_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/nb_pfi_corr.csv')\n",
        "# nnmlp_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/nnmlp_pfi_corr.csv')\n",
        "# svc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/svc_pfi_corr.csv')\n",
        "# lstm_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/lstm_pfi_corr.csv')\n",
        "# cnn_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/cnn_pfi_corr.csv')\n",
        "\n",
        "# gbc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/gbc_pfi_mis.csv')\n",
        "# ada_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/ada_pfi_mis.csv')\n",
        "# rfc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/rfc_pfi_mis.csv')\n",
        "# nb_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/nb_pfi_mis.csv')\n",
        "# nnmlp_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/nnmlp_pfi_mis.csv')\n",
        "# svc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/svc_pfi_mis.csv')\n",
        "# lstm_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/lstm_pfi_mis.csv')\n",
        "# cnn_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/cnn_pfi_mis.csv')\n",
        "\n",
        "# gbc_pfi_corr = gbc_pfi_corr.weight.values\n",
        "# ada_pfi_corr = ada_pfi_corr.weight.values\n",
        "# rfc_pfi_corr = rfc_pfi_corr.weight.values\n",
        "# nb_pfi_corr = nb_pfi_corr.weight.values\n",
        "# nnmlp_pfi_corr = nnmlp_pfi_corr.weight.values\n",
        "# svc_pfi_corr = svc_pfi_corr.weight.values\n",
        "# lstm_pfi_corr = lstm_pfi_corr.weight.values\n",
        "# cnn_pfi_corr = cnn_pfi_corr.weight.values\n",
        "\n",
        "# gbc_pfi_mis = gbc_pfi_mis.weight.values\n",
        "# ada_pfi_mis = ada_pfi_mis.weight.values\n",
        "# rfc_pfi_mis = rfc_pfi_mis.weight.values\n",
        "# nb_pfi_mis = nb_pfi_mis.weight.values\n",
        "# nnmlp_pfi_mis = nnmlp_pfi_mis.weight.values\n",
        "# svc_pfi_mis = svc_pfi_mis.weight.values\n",
        "# lstm_pfi_mis = lstm_pfi_mis.weight.values\n",
        "# cnn_pfi_mis = cnn_pfi_mis.weight.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ZWG1t9LkHN"
      },
      "source": [
        "#permutation feature importance values\n",
        "gbc_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/gbc_pfi.csv')\n",
        "ada_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/ada_pfi.csv')\n",
        "rfc_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/rfc_pfi.csv')\n",
        "nb_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nb_pfi.csv')\n",
        "nnmlp_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nnmlp_pfi.csv')\n",
        "svc_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/svc_pfi.csv')\n",
        "lstm_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/lstm_pfi.csv')\n",
        "cnn_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/cnn_pfi.csv')\n",
        "\n",
        "gbc_pfi = gbc_pfi.weight.values\n",
        "ada_pfi = ada_pfi.weight.values\n",
        "rfc_pfi = rfc_pfi.weight.values\n",
        "nb_pfi = nb_pfi.weight.values\n",
        "nnmlp_pfi = nnmlp_pfi.weight.values\n",
        "svc_pfi = svc_pfi.weight.values\n",
        "lstm_pfi = lstm_pfi.weight.values\n",
        "cnn_pfi = cnn_pfi.weight.values\n",
        "\n",
        "gbc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/gbc_pfi_corr.csv')\n",
        "ada_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/ada_pfi_corr.csv')\n",
        "rfc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/rfc_pfi_corr.csv')\n",
        "nb_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nb_pfi_corr.csv')\n",
        "nnmlp_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nnmlp_pfi_corr.csv')\n",
        "svc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/svc_pfi_corr.csv')\n",
        "lstm_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/lstm_pfi_corr.csv')\n",
        "cnn_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/cnn_pfi_corr.csv')\n",
        "\n",
        "gbc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/gbc_pfi_mis.csv')\n",
        "ada_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/ada_pfi_mis.csv')\n",
        "rfc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/rfc_pfi_mis.csv')\n",
        "nb_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nb_pfi_mis.csv')\n",
        "nnmlp_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nnmlp_pfi_mis.csv')\n",
        "svc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/svc_pfi_mis.csv')\n",
        "lstm_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/lstm_pfi_mis.csv')\n",
        "cnn_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/cnn_pfi_mis.csv')\n",
        "\n",
        "gbc_pfi_corr = gbc_pfi_corr.weight.values\n",
        "ada_pfi_corr = ada_pfi_corr.weight.values\n",
        "rfc_pfi_corr = rfc_pfi_corr.weight.values\n",
        "nb_pfi_corr = nb_pfi_corr.weight.values\n",
        "nnmlp_pfi_corr = nnmlp_pfi_corr.weight.values\n",
        "svc_pfi_corr = svc_pfi_corr.weight.values\n",
        "lstm_pfi_corr = lstm_pfi_corr.weight.values\n",
        "cnn_pfi_corr = cnn_pfi_corr.weight.values\n",
        "\n",
        "gbc_pfi_mis = gbc_pfi_mis.weight.values\n",
        "ada_pfi_mis = ada_pfi_mis.weight.values\n",
        "rfc_pfi_mis = rfc_pfi_mis.weight.values\n",
        "nb_pfi_mis = nb_pfi_mis.weight.values\n",
        "nnmlp_pfi_mis = nnmlp_pfi_mis.weight.values\n",
        "svc_pfi_mis = svc_pfi_mis.weight.values\n",
        "lstm_pfi_mis = lstm_pfi_mis.weight.values\n",
        "cnn_pfi_mis = cnn_pfi_mis.weight.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3HyWz9lTTnd"
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import *\n",
        "\n",
        "\n",
        "# use this for holdout 25/75 >>>>>>>>>>>>>>>.\n",
        "train_values = np.empty(shape=[0, 222])\n",
        "test_values = np.empty(shape=[0, 222])\n",
        "\n",
        "train_sampled_all = glob.glob('./drive/MyDrive/compsci/train_sampled_all_220.csv')\n",
        "test_unsampled_all = glob.glob('./drive/MyDrive/compsci/test_unsampled_all_220.csv')\n",
        "\n",
        "for j in train_sampled_all:\n",
        "    print('Loading ', j)\n",
        "    csvrows = np.loadtxt(j, delimiter=',')\n",
        "    train_values = np.append(train_values, csvrows, axis=0)\n",
        "\n",
        "for j in test_unsampled_all:\n",
        "    print('Loading ', j)\n",
        "    csvrows = np.loadtxt(j, delimiter=',')\n",
        "    test_values = np.append(test_values, csvrows, axis=0)\n",
        "    \n",
        "print(train_values.shape)\n",
        "print(test_values.shape)\n",
        "\n",
        "# # use this for leave patients >>>>>>>>>>>>>>>.\n",
        "\n",
        "# train_values = np.empty(shape=[0, 222])\n",
        "# test_values = np.empty(shape=[0, 222])\n",
        "\n",
        "# train_patients = glob.glob('./drive/MyDrive/compsci/train_patients.csv')\n",
        "# test_patients = glob.glob('./drive/MyDrive/compsci/test_patients.csv')\n",
        "\n",
        "# for j in train_patients:\n",
        "#     print('Loading ', j)\n",
        "#     csvrows = np.loadtxt(j, delimiter=',')\n",
        "#     train_values = np.append(train_values, csvrows, axis=0)\n",
        "\n",
        "# for j in test_patients:\n",
        "#     print('Loading ', j)\n",
        "#     csvrows = np.loadtxt(j, delimiter=',')\n",
        "#     test_values = np.append(test_values, csvrows, axis=0)\n",
        "    \n",
        "# print(train_values.shape)\n",
        "# print(test_values.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68cnzu9cTYOq"
      },
      "source": [
        "X_train = train_values[:,:-2]\n",
        "X_test = test_values[:,:-2]\n",
        "y_train = train_values[:,-2]\n",
        "y_test = test_values[:,-2]\n",
        "y_train_gc = (y_train - y_train.min())/(y_train.max()-y_train.min())*(9-1)\n",
        "y_test_gc = (y_test - y_test.min())/(y_test.max()-y_test.min())*(9-1)\n",
        "X_train1 = X_train.reshape(X_train.shape + (1,1))\n",
        "X_test1 = X_test.reshape(X_test.shape + (1,1))\n",
        "y_train1=to_categorical(y_train)\n",
        "y_test1=to_categorical(y_test)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbC5hR6mThqS"
      },
      "source": [
        "# verbose, epoch, batch_size = 1, 5, 256\n",
        "# activationFunction='relu'\n",
        "\n",
        "# def getlstmModel():\n",
        "    \n",
        "#     lstmmodel = Sequential()\n",
        "#     lstmmodel.add(LSTM(128, return_sequences=True, input_shape=(X_train1.shape[1],1)))\n",
        "#     lstmmodel.add(LSTM(9, return_sequences=True))\n",
        "#     lstmmodel.add(MaxPooling1D(pool_size=2))\n",
        "#     lstmmodel.add(Flatten())\n",
        "#     lstmmodel.add(Dense(512, activation=tf.nn.relu))    \n",
        "#     lstmmodel.add(Dense(128, activation=tf.nn.relu))    \n",
        "#     lstmmodel.add(Dense(32, activation=tf.nn.relu))\n",
        "#     lstmmodel.add(Dense(9, activation='softmax'))\n",
        "#     lstmmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#     lstmmodel.summary()\n",
        "#     return lstmmodel\n",
        "\n",
        "# lstmmodel = getlstmModel()\n",
        "\n",
        "# lstmmodelhistory= lstmmodel.fit(X_train1[:,:,:,0], y_train1, epochs=epoch, verbose=verbose, validation_split=0.2, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7Ha-deTocR"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "#loading the model and running it on datasets\n",
        "from keras.models import load_model\n",
        "\n",
        "lstmmodel = load_model('/content/drive/MyDrive/compsci/modelsresult/lstmmodel.h5')\n",
        "lstmmodel.summary()\n",
        "# lstm_json = open('/content/drive/MyDrive/compsci/modelsresult/lstmmodel_weights.json', 'r')\n",
        "# loaded_lstmmodel_json = lstm_json.read()\n",
        "# lstm_json.close()\n",
        "# loaded_lstm_model = model_from_json(loaded_lstmmodel_json)\n",
        "# # load weights into new model\n",
        "# loaded_lstm_model.load_weights(\"/content/drive/MyDrive/compsci/modelsresult/lstmmodel_weights.h5\")\n",
        "\n",
        "get_last_conv1 = keras.backend.function([lstmmodel.layers[0].input, keras.backend.learning_phase()], [lstmmodel.layers[1].output])\n",
        "last_conv1_lstm = get_last_conv1([X_test1[:,:,:,0]])[0]\n",
        "\n",
        "get_softmax1_lstm = keras.backend.function([lstmmodel.layers[0].input, keras.backend.learning_phase()], [lstmmodel.layers[-1].output])\n",
        "softmax1_lstm = get_softmax1_lstm(([X_test1[:,:,:,0]]))[0]\n",
        "softmax_weight1_lstm = lstmmodel.get_weights()[-1]\n",
        "softmax_weight_lstm = np.reshape(softmax_weight1_lstm,(9,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpfD9Je0TvnN"
      },
      "source": [
        "from itertools import islice\n",
        "def means_of_slices(iterable, slice_size):\n",
        "    iterator = iter(iterable)\n",
        "    while True:\n",
        "        slice = list(islice(iterator, slice_size))\n",
        "        if slice:\n",
        "            yield np.sum(slice)/len(slice)\n",
        "        else:\n",
        "            return\n",
        "a = last_conv1_lstm\n",
        "new_last_lstm = []\n",
        "for i in range(len(last_conv1_lstm)):\n",
        "  means = list(means_of_slices(a[i], 20))\n",
        "  new_last_lstm.append(means)\n",
        "new_last_lstm = np.array(new_last_lstm)\n",
        "print(new_last_lstm.shape)\n",
        "\n",
        "final_last_lstm = []\n",
        "for i in new_last_lstm:\n",
        "  final_last_lstm.append(np.repeat(i,20))\n",
        "final_last_lstm = np.array(final_last_lstm)\n",
        "print(final_last_lstm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgJj-s3DT19K"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# verbose, epoch, batch_size = 1, 5, 64\n",
        "# activationFunction='relu'\n",
        "\n",
        "# def getModel():\n",
        "    \n",
        "#     cnnmodel = Sequential()\n",
        "#     cnnmodel.add(Conv1D(filters=128, kernel_size=16,padding='same', activation='relu',input_shape=(X_train1.shape[1],1)))\n",
        "#     cnnmodel.add(BatchNormalization())\n",
        "#     cnnmodel.add(Conv1D(filters=32, kernel_size=16,padding='same', activation='relu'))\n",
        "#     cnnmodel.add(BatchNormalization())\n",
        "#     cnnmodel.add(Conv1D(filters=9, kernel_size=16,padding='same', activation='relu'))\n",
        "#     cnnmodel.add(MaxPooling1D(pool_size=2,padding='same'))\n",
        "#     cnnmodel.add(Flatten())\n",
        "#     cnnmodel.add(Dense(512, activation='relu'))\n",
        "#     cnnmodel.add(Dense(128, activation='relu'))\n",
        "#     cnnmodel.add(Dense(32, activation='relu'))\n",
        "#     cnnmodel.add(Dense(9, activation='softmax'))\n",
        "#     cnnmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#     cnnmodel.summary()\n",
        "#     return cnnmodel\n",
        "\n",
        "# cnnmodel = getModel()\n",
        "# modelhistory= cnnmodel.fit(X_train1[:,:,:,0], y_train1, epochs=epoch, verbose=verbose, validation_split=0.2, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjQedQ7mT4iK"
      },
      "source": [
        "cnnmodel = load_model('/content/drive/MyDrive/compsci/modelsresult/cnnmodel.h5')\n",
        "cnnmodel.summary()\n",
        "# cnn_json = open('/content/drive/MyDrive/compsci/modelsresult/cnnmodel_weights.json', 'r')\n",
        "# loaded_cnnmodel_json = cnn_json.read()\n",
        "# cnn_json.close()\n",
        "# loaded_cnn_model = model_from_json(loaded_cnnmodel_json)\n",
        "# # load weights into new model\n",
        "# loaded_cnn_model.load_weights(\"/content/drive/MyDrive/compsci/modelsresult/cnnmodel_weights.h5\")\n",
        "\n",
        "get_last_conv1 = keras.backend.function([cnnmodel.layers[0].input, keras.backend.learning_phase()], [cnnmodel.layers[4].output])\n",
        "last_conv1_cnn = get_last_conv1([X_test1[:,:,:,0]])[0]\n",
        "\n",
        "get_softmax1_cnn = keras.backend.function([cnnmodel.layers[0].input, keras.backend.learning_phase()], [cnnmodel.layers[-1].output])\n",
        "softmax1_cnn = get_softmax1_cnn(([X_test1[:,:,:,0]]))[0]\n",
        "softmax_weight1_cnn = cnnmodel.get_weights()[-1]\n",
        "softmax_weight_cnn = np.reshape(softmax_weight1_cnn,(9,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox-hCgl5T9Hk"
      },
      "source": [
        "from itertools import islice\n",
        "def means_of_slices(iterable, slice_size):\n",
        "    iterator = iter(iterable)\n",
        "    while True:\n",
        "        slice = list(islice(iterator, slice_size))\n",
        "        if slice:\n",
        "            yield np.sum(slice)/len(slice)\n",
        "        else:\n",
        "            return\n",
        "a = last_conv1_cnn\n",
        "new_last_conv1 = []\n",
        "\n",
        "for i in range(len(last_conv1_cnn)):\n",
        "  means = list(means_of_slices(a[i], 20))\n",
        "  new_last_conv1.append(means)\n",
        "new_last_conv1 = np.array(new_last_conv1)\n",
        "print(new_last_conv1.shape)\n",
        "\n",
        "final_last_conv1 = []\n",
        "for i in new_last_conv1:\n",
        "  final_last_conv1.append(np.repeat(i,20))\n",
        "final_last_conv1 = np.array(final_last_conv1)\n",
        "print(final_last_conv1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG8OhI_PUCXS"
      },
      "source": [
        "df_cnn = pd.DataFrame(new_last_conv1)\n",
        "corr_cnn = df_cnn.corr(method='kendall')\n",
        "slices_nums = [1,2,3,4,5,6,7,8,9,10,11]\n",
        "rcParams['figure.figsize'] = 8,6\n",
        "sns.heatmap(corr_cnn,annot=True,xticklabels=slices_nums,yticklabels=slices_nums,vmin=-0.6,vmax=1,fmt='0.1g')\n",
        "plt.title('CNN Kendall Tau Corellation')\n",
        "plt.show()\n",
        "\n",
        "df_lstm = pd.DataFrame(new_last_lstm)\n",
        "corr_lstm = df_lstm.corr(method='kendall')\n",
        "slices_nums = [1,2,3,4,5,6,7,8,9,10,11]\n",
        "rcParams['figure.figsize'] = 8,6\n",
        "sns.heatmap(corr_lstm,annot=True,xticklabels=slices_nums,yticklabels=slices_nums,vmin=-0.6,vmax=1,fmt='0.1g')\n",
        "plt.title('LSTM Kendall Tau Corellation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDVz_Yl7yA1i"
      },
      "source": [
        "from itertools import islice\n",
        "def means_of_slices(iterable, slice_size):\n",
        "    iterator = iter(iterable)\n",
        "    while True:\n",
        "        slice = list(islice(iterator, slice_size))\n",
        "        if slice:\n",
        "            yield np.sum(slice)/len(slice)\n",
        "        else:\n",
        "            return\n",
        "\n",
        "a = X_train[:,:]\n",
        "print(a.shape)\n",
        "b = []\n",
        "\n",
        "for i in range(90):\n",
        "  means = list(means_of_slices(a[i], 20))\n",
        "  b.append(means)\n",
        "b = np.array(b)\n",
        "print(b.shape)\n",
        "\n",
        "c = []\n",
        "for i in b:\n",
        "  c.append(np.repeat(i,20))\n",
        "c = np.array(c)\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpw78gUiUEa7"
      },
      "source": [
        "# df_cnn_std = (df_cnn-df_cnn.mean())/df_cnn.std()\n",
        "# df_lstm_std = (df_lstm-df_lstm.mean())/df_lstm.std()\n",
        "rcParams['figure.figsize'] = 8,6\n",
        "\n",
        "corrwith = df_cnn.corrwith(df_lstm, method='kendall')\n",
        "print(corrwith)\n",
        "\n",
        "corrwith.plot(kind='bar',rot=0, label = 'Correlations')\n",
        "plt.plot(b[50]/6, color='r',label = 'Sample ECG')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('segments')\n",
        "plt.ylabel('correlation')\n",
        "plt.title('Kendall Tau Correlation Between CNN and LSTM Segments')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4csiK3pin3k"
      },
      "source": [
        "models = [gbc_pfi,ada_pfi,rfc_pfi,nb_pfi,nnmlp_pfi,svc_pfi,cnn_pfi,lstm_pfi]\n",
        "models = pd.DataFrame(models)\n",
        "models = models.transpose()\n",
        "models.columns = ['GBC','ADA','RFC','NB','NNMLP','SVC','CNN','LSTM']\n",
        "models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTDCvtDsQqPF"
      },
      "source": [
        "models_corr = [gbc_pfi_corr,ada_pfi_corr,rfc_pfi_corr,nb_pfi_corr,nnmlp_pfi_corr,svc_pfi_corr,cnn_pfi_corr,lstm_pfi_corr]\n",
        "models_corr = pd.DataFrame(models_corr)\n",
        "models_corr = models_corr.transpose()\n",
        "models_corr.columns = ['GBC','ADA','RFC','NB','NNMLP','SVC','CNN','LSTM']\n",
        "models_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSKjwvcRQ14I"
      },
      "source": [
        "models_mis = [gbc_pfi_mis,ada_pfi_mis,rfc_pfi_mis,nb_pfi_mis,nnmlp_pfi_mis,svc_pfi_mis,cnn_pfi_mis,lstm_pfi_mis]\n",
        "models_mis = pd.DataFrame(models_mis)\n",
        "models_mis = models_mis.transpose()\n",
        "models_mis.columns = ['GBC','ADA','RFC','NB','NNMLP','SVC','CNN','LSTM']\n",
        "models_mis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9tIaBQWivZT"
      },
      "source": [
        "corr = models.corr(method='kendall')\n",
        "rcParams['figure.figsize'] = 8,6\n",
        "sns.heatmap(corr,annot=True,fmt='0.1g')\n",
        "plt.title('Kendall Tau Corellations Between Models')\n",
        "plt.show()\n",
        "\n",
        "corr_corr = models_corr.corr(method='kendall')\n",
        "rcParams['figure.figsize'] = 8,6\n",
        "sns.heatmap(corr_corr,annot=True,fmt='0.1g')\n",
        "plt.title('Kendall Tau Corellations Between Correct Models')\n",
        "plt.show()\n",
        "\n",
        "corr_mis = models_mis.corr(method='kendall')\n",
        "rcParams['figure.figsize'] = 8,6\n",
        "sns.heatmap(corr_mis,annot=True,fmt='0.1g')\n",
        "plt.title('Kendall Tau Corellations Between Misclass Models')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlyMnFZfOi3"
      },
      "source": [
        "from sklearn.metrics import *\n",
        "def showResults(test, pred, model_name):\n",
        "    #target_names = ['positive', 'negative']\n",
        "    # print(classification_report(test, pred, target_names=target_names))\n",
        "    accuracy = accuracy_score(test, pred)\n",
        "    precision= precision_score(test, pred, average='weighted')\n",
        "    recall = recall_score(test, pred, average = 'weighted')\n",
        "    f1score= f1_score(test, pred, average='weighted') \n",
        "    #loss=log_loss(test,pred)\n",
        "    print(\"Accuracy  : {}\".format(accuracy))\n",
        "    print(\"Precision : {}\".format(precision))\n",
        "    print(\"Recall : {}\".format(recall))\n",
        "    print(\"f1score : {}\".format(f1score))\n",
        "    #print(\"Loss : {}\".format(loss))\n",
        "    cm=confusion_matrix(test, pred,labels=[1,2,3,4,5,6,7,8])\n",
        "    cm = np.nan_to_num(cm)\n",
        "    print(cm)\n",
        "    return (model_name, round(accuracy,3), round(precision,3) , round(recall,3) , round(f1score,3), cm)\n",
        "\n",
        "cnnpredictions = cnnmodel.predict(X_test1[:,:,:,0], verbose=1)\n",
        "\n",
        "cnn_predict=np.argmax(cnnpredictions,axis=1)\n",
        "cnn_actual_value=np.argmax(y_test1,axis=1)\n",
        "cnn_results = showResults(cnn_actual_value, cnn_predict,'CNN')\n",
        "from sklearn import metrics\n",
        "cnnmetrics = metrics.classification_report(cnn_actual_value, cnn_predict, digits=3)\n",
        "print(cnnmetrics, 'CNN metrics')\n",
        "categories=['N','L','R','V','A','F','f','/']\n",
        "plt.figure(figsize=(8,6))\n",
        "CNN_cf_m = cnn_results[5].astype('float')/ cnn_results[5].sum(axis=1)[:,np.newaxis]\n",
        "sns.heatmap(np.nan_to_num(CNN_cf_m),annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)\n",
        "plt.title('1D-CNN confusion matrix')\n",
        "plt.show()\n",
        "\n",
        "lstmpredictions = lstmmodel.predict(X_test1[:,:,:,0], verbose=1)\n",
        "\n",
        "lstm_predict=np.argmax(lstmpredictions,axis=1)\n",
        "lstm_actual_value=np.argmax(y_test1,axis=1)\n",
        "lstm_results = showResults(lstm_actual_value, lstm_predict,'LSTM')\n",
        "from sklearn import metrics\n",
        "lstmmetrics = metrics.classification_report(lstm_actual_value, lstm_predict, digits=3)\n",
        "print(lstmmetrics, 'lstm metrics')\n",
        "categories=['N','L','R','V','A','F','f','/']\n",
        "plt.figure(figsize=(8,6))\n",
        "lstm_cf_m = lstm_results[5].astype('float')/ lstm_results[5].sum(axis=1)[:,np.newaxis]\n",
        "sns.heatmap(np.nan_to_num(lstm_cf_m),annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)\n",
        "plt.title('LSTM confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}