# -*- coding: utf-8 -*-
"""correlation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tkcszzyvm8gZffZ4JuQ4pGmBix9SKhYD
"""

import numpy as np
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
from pylab import rcParams
from scipy.stats.stats import kendalltau
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from keras.utils.np_utils import to_categorical
from sklearn.utils import class_weight
from sklearn.metrics import log_loss
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Conv1D, Conv1DTranspose
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import LSTM, Masking
from tensorflow.keras.layers import MaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras import activations
sns.set()

# #permutation feature importance values
# gbc_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/gbc_pfi.csv')
# ada_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/ada_pfi.csv')
# rfc_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/rfc_pfi.csv')
# nb_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/nb_pfi.csv')
# nnmlp_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/nnmlp_pfi.csv')
# svc_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/svc_pfi.csv')
# lstm_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/lstm_pfi.csv')
# cnn_pfi = pd.read_csv('./drive/MyDrive/compsci/pfi/cnn_pfi.csv')

# gbc_pfi = gbc_pfi.weight.values
# ada_pfi = ada_pfi.weight.values
# rfc_pfi = rfc_pfi.weight.values
# nb_pfi = nb_pfi.weight.values
# nnmlp_pfi = nnmlp_pfi.weight.values
# svc_pfi = svc_pfi.weight.values
# lstm_pfi = lstm_pfi.weight.values
# cnn_pfi = cnn_pfi.weight.values

# gbc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/gbc_pfi_corr.csv')
# ada_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/ada_pfi_corr.csv')
# rfc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/rfc_pfi_corr.csv')
# nb_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/nb_pfi_corr.csv')
# nnmlp_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/nnmlp_pfi_corr.csv')
# svc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/svc_pfi_corr.csv')
# lstm_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/lstm_pfi_corr.csv')
# cnn_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/pfi/cnn_pfi_corr.csv')

# gbc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/gbc_pfi_mis.csv')
# ada_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/ada_pfi_mis.csv')
# rfc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/rfc_pfi_mis.csv')
# nb_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/nb_pfi_mis.csv')
# nnmlp_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/nnmlp_pfi_mis.csv')
# svc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/svc_pfi_mis.csv')
# lstm_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/lstm_pfi_mis.csv')
# cnn_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/pfi/cnn_pfi_mis.csv')

# gbc_pfi_corr = gbc_pfi_corr.weight.values
# ada_pfi_corr = ada_pfi_corr.weight.values
# rfc_pfi_corr = rfc_pfi_corr.weight.values
# nb_pfi_corr = nb_pfi_corr.weight.values
# nnmlp_pfi_corr = nnmlp_pfi_corr.weight.values
# svc_pfi_corr = svc_pfi_corr.weight.values
# lstm_pfi_corr = lstm_pfi_corr.weight.values
# cnn_pfi_corr = cnn_pfi_corr.weight.values

# gbc_pfi_mis = gbc_pfi_mis.weight.values
# ada_pfi_mis = ada_pfi_mis.weight.values
# rfc_pfi_mis = rfc_pfi_mis.weight.values
# nb_pfi_mis = nb_pfi_mis.weight.values
# nnmlp_pfi_mis = nnmlp_pfi_mis.weight.values
# svc_pfi_mis = svc_pfi_mis.weight.values
# lstm_pfi_mis = lstm_pfi_mis.weight.values
# cnn_pfi_mis = cnn_pfi_mis.weight.values

#permutation feature importance values
gbc_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/gbc_pfi.csv')
ada_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/ada_pfi.csv')
rfc_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/rfc_pfi.csv')
nb_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nb_pfi.csv')
nnmlp_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nnmlp_pfi.csv')
svc_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/svc_pfi.csv')
lstm_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/lstm_pfi.csv')
cnn_pfi = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/cnn_pfi.csv')

gbc_pfi = gbc_pfi.weight.values
ada_pfi = ada_pfi.weight.values
rfc_pfi = rfc_pfi.weight.values
nb_pfi = nb_pfi.weight.values
nnmlp_pfi = nnmlp_pfi.weight.values
svc_pfi = svc_pfi.weight.values
lstm_pfi = lstm_pfi.weight.values
cnn_pfi = cnn_pfi.weight.values

gbc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/gbc_pfi_corr.csv')
ada_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/ada_pfi_corr.csv')
rfc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/rfc_pfi_corr.csv')
nb_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nb_pfi_corr.csv')
nnmlp_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nnmlp_pfi_corr.csv')
svc_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/svc_pfi_corr.csv')
lstm_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/lstm_pfi_corr.csv')
cnn_pfi_corr = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/cnn_pfi_corr.csv')

gbc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/gbc_pfi_mis.csv')
ada_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/ada_pfi_mis.csv')
rfc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/rfc_pfi_mis.csv')
nb_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nb_pfi_mis.csv')
nnmlp_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/nnmlp_pfi_mis.csv')
svc_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/svc_pfi_mis.csv')
lstm_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/lstm_pfi_mis.csv')
cnn_pfi_mis = pd.read_csv('./drive/MyDrive/compsci/leave_pfi/cnn_pfi_mis.csv')

gbc_pfi_corr = gbc_pfi_corr.weight.values
ada_pfi_corr = ada_pfi_corr.weight.values
rfc_pfi_corr = rfc_pfi_corr.weight.values
nb_pfi_corr = nb_pfi_corr.weight.values
nnmlp_pfi_corr = nnmlp_pfi_corr.weight.values
svc_pfi_corr = svc_pfi_corr.weight.values
lstm_pfi_corr = lstm_pfi_corr.weight.values
cnn_pfi_corr = cnn_pfi_corr.weight.values

gbc_pfi_mis = gbc_pfi_mis.weight.values
ada_pfi_mis = ada_pfi_mis.weight.values
rfc_pfi_mis = rfc_pfi_mis.weight.values
nb_pfi_mis = nb_pfi_mis.weight.values
nnmlp_pfi_mis = nnmlp_pfi_mis.weight.values
svc_pfi_mis = svc_pfi_mis.weight.values
lstm_pfi_mis = lstm_pfi_mis.weight.values
cnn_pfi_mis = cnn_pfi_mis.weight.values

import numpy as np
import glob
import matplotlib.pyplot as plt
import pandas as pd
from scipy import *


# use this for holdout 25/75 >>>>>>>>>>>>>>>.
train_values = np.empty(shape=[0, 222])
test_values = np.empty(shape=[0, 222])

train_sampled_all = glob.glob('./drive/MyDrive/compsci/train_sampled_all_220.csv')
test_unsampled_all = glob.glob('./drive/MyDrive/compsci/test_unsampled_all_220.csv')

for j in train_sampled_all:
    print('Loading ', j)
    csvrows = np.loadtxt(j, delimiter=',')
    train_values = np.append(train_values, csvrows, axis=0)

for j in test_unsampled_all:
    print('Loading ', j)
    csvrows = np.loadtxt(j, delimiter=',')
    test_values = np.append(test_values, csvrows, axis=0)
    
print(train_values.shape)
print(test_values.shape)

# # use this for leave patients >>>>>>>>>>>>>>>.

# train_values = np.empty(shape=[0, 222])
# test_values = np.empty(shape=[0, 222])

# train_patients = glob.glob('./drive/MyDrive/compsci/train_patients.csv')
# test_patients = glob.glob('./drive/MyDrive/compsci/test_patients.csv')

# for j in train_patients:
#     print('Loading ', j)
#     csvrows = np.loadtxt(j, delimiter=',')
#     train_values = np.append(train_values, csvrows, axis=0)

# for j in test_patients:
#     print('Loading ', j)
#     csvrows = np.loadtxt(j, delimiter=',')
#     test_values = np.append(test_values, csvrows, axis=0)
    
# print(train_values.shape)
# print(test_values.shape)

X_train = train_values[:,:-2]
X_test = test_values[:,:-2]
y_train = train_values[:,-2]
y_test = test_values[:,-2]
y_train_gc = (y_train - y_train.min())/(y_train.max()-y_train.min())*(9-1)
y_test_gc = (y_test - y_test.min())/(y_test.max()-y_test.min())*(9-1)
X_train1 = X_train.reshape(X_train.shape + (1,1))
X_test1 = X_test.reshape(X_test.shape + (1,1))
y_train1=to_categorical(y_train)
y_test1=to_categorical(y_test)

import tensorflow as tf
tf.compat.v1.disable_eager_execution()

verbose, epoch, batch_size = 1, 5, 256
activationFunction='relu'

def getlstmModel():
    
    lstmmodel = Sequential()
    lstmmodel.add(LSTM(128, return_sequences=True, input_shape=(X_train1.shape[1],1)))
    lstmmodel.add(LSTM(9, return_sequences=True))
    lstmmodel.add(MaxPooling1D(pool_size=2))
    lstmmodel.add(Flatten())
    lstmmodel.add(Dense(512, activation=tf.nn.relu))    
    lstmmodel.add(Dense(128, activation=tf.nn.relu))    
    lstmmodel.add(Dense(32, activation=tf.nn.relu))
    lstmmodel.add(Dense(9, activation='softmax'))
    lstmmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
    lstmmodel.summary()
    return lstmmodel

lstmmodel = getlstmModel()

lstmmodelhistory= lstmmodel.fit(X_train1[:,:,:,0], y_train1, epochs=epoch, verbose=verbose, validation_split=0.2, batch_size = batch_size)

get_last_conv1 = keras.backend.function([lstmmodel.layers[0].input, keras.backend.learning_phase()], [lstmmodel.layers[1].output])
last_conv1_lstm = get_last_conv1([X_test1[:,:,:,0]])[0]

get_softmax1_lstm = keras.backend.function([lstmmodel.layers[0].input, keras.backend.learning_phase()], [lstmmodel.layers[-1].output])
softmax1_lstm = get_softmax1_lstm(([X_test1[:,:,:,0]]))[0]
softmax_weight1_lstm = lstmmodel.get_weights()[-1]
softmax_weight_lstm = np.reshape(softmax_weight1_lstm,(9,1))

from itertools import islice
def means_of_slices(iterable, slice_size):
    iterator = iter(iterable)
    while True:
        slice = list(islice(iterator, slice_size))
        if slice:
            yield np.sum(slice)/len(slice)
        else:
            return
a = last_conv1_lstm
new_last_lstm = []
for i in range(len(last_conv1_lstm)):
  means = list(means_of_slices(a[i], 20))
  new_last_lstm.append(means)
new_last_lstm = np.array(new_last_lstm)
print(new_last_lstm.shape)

final_last_lstm = []
for i in new_last_lstm:
  final_last_lstm.append(np.repeat(i,20))
final_last_lstm = np.array(final_last_lstm)
print(final_last_lstm.shape)

import tensorflow as tf
tf.compat.v1.disable_eager_execution()

verbose, epoch, batch_size = 1, 5, 64
activationFunction='relu'

def getModel():
    
    cnnmodel = Sequential()
    cnnmodel.add(Conv1D(filters=128, kernel_size=16,padding='same', activation='relu',input_shape=(X_train1.shape[1],1)))
    cnnmodel.add(BatchNormalization())
    cnnmodel.add(Conv1D(filters=32, kernel_size=16,padding='same', activation='relu'))
    cnnmodel.add(BatchNormalization())
    cnnmodel.add(Conv1D(filters=9, kernel_size=16,padding='same', activation='relu'))
    cnnmodel.add(MaxPooling1D(pool_size=2,padding='same'))
    cnnmodel.add(Flatten())
    cnnmodel.add(Dense(512, activation='relu'))
    cnnmodel.add(Dense(128, activation='relu'))
    cnnmodel.add(Dense(32, activation='relu'))
    cnnmodel.add(Dense(9, activation='softmax'))
    cnnmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
    cnnmodel.summary()
    return cnnmodel

cnnmodel = getModel()
modelhistory= cnnmodel.fit(X_train1[:,:,:,0], y_train1, epochs=epoch, verbose=verbose, validation_split=0.2, batch_size = batch_size)

get_last_conv1 = keras.backend.function([cnnmodel.layers[0].input, keras.backend.learning_phase()], [cnnmodel.layers[4].output])
last_conv1_cnn = get_last_conv1([X_test1[:,:,:,0]])[0]

get_softmax1_cnn = keras.backend.function([cnnmodel.layers[0].input, keras.backend.learning_phase()], [cnnmodel.layers[-1].output])
softmax1_cnn = get_softmax1_cnn(([X_test1[:,:,:,0]]))[0]
softmax_weight1_cnn = cnnmodel.get_weights()[-1]
softmax_weight_cnn = np.reshape(softmax_weight1_cnn,(9,1))

from itertools import islice
def means_of_slices(iterable, slice_size):
    iterator = iter(iterable)
    while True:
        slice = list(islice(iterator, slice_size))
        if slice:
            yield np.sum(slice)/len(slice)
        else:
            return
a = last_conv1_cnn
new_last_conv1 = []

for i in range(len(last_conv1_cnn)):
  means = list(means_of_slices(a[i], 20))
  new_last_conv1.append(means)
new_last_conv1 = np.array(new_last_conv1)
print(new_last_conv1.shape)

final_last_conv1 = []
for i in new_last_conv1:
  final_last_conv1.append(np.repeat(i,20))
final_last_conv1 = np.array(final_last_conv1)
print(final_last_conv1.shape)

df_cnn = pd.DataFrame(new_last_conv1)
corr_cnn = df_cnn.corr(method='kendall')
slices_nums = [1,2,3,4,5,6,7,8,9,10,11]
rcParams['figure.figsize'] = 8,6
sns.heatmap(corr_cnn,annot=True,xticklabels=slices_nums,yticklabels=slices_nums,vmin=-0.6,vmax=1,fmt='0.1g')
plt.title('CNN Kendall Tau Corellation')
plt.show()

df_lstm = pd.DataFrame(new_last_lstm)
corr_lstm = df_lstm.corr(method='kendall')
slices_nums = [1,2,3,4,5,6,7,8,9,10,11]
rcParams['figure.figsize'] = 8,6
sns.heatmap(corr_lstm,annot=True,xticklabels=slices_nums,yticklabels=slices_nums,vmin=-0.6,vmax=1,fmt='0.1g')
plt.title('LSTM Kendall Tau Corellation')
plt.show()

from itertools import islice
def means_of_slices(iterable, slice_size):
    iterator = iter(iterable)
    while True:
        slice = list(islice(iterator, slice_size))
        if slice:
            yield np.sum(slice)/len(slice)
        else:
            return

a = X_train[:,:]
print(a.shape)
b = []

for i in range(90):
  means = list(means_of_slices(a[i], 20))
  b.append(means)
b = np.array(b)
print(b.shape)

c = []
for i in b:
  c.append(np.repeat(i,20))
c = np.array(c)
print(c.shape)

# df_cnn_std = (df_cnn-df_cnn.mean())/df_cnn.std()
# df_lstm_std = (df_lstm-df_lstm.mean())/df_lstm.std()
rcParams['figure.figsize'] = 8,6

corrwith = df_cnn.corrwith(df_lstm, method='kendall')
print(corrwith)

corrwith.plot(kind='bar',rot=0, label = 'Correlations')
plt.plot(b[50]/6, color='r',label = 'Sample ECG')

plt.legend()

plt.xlabel('segments')
plt.ylabel('correlation')
plt.title('Kendall Tau Correlation Between CNN and LSTM Segments')

models = [gbc_pfi,ada_pfi,rfc_pfi,nb_pfi,nnmlp_pfi,svc_pfi,cnn_pfi,lstm_pfi]
models = pd.DataFrame(models)
models = models.transpose()
models.columns = ['GBC','ADA','RFC','NB','NNMLP','SVC','CNN','LSTM']
models

models_corr = [gbc_pfi_corr,ada_pfi_corr,rfc_pfi_corr,nb_pfi_corr,nnmlp_pfi_corr,svc_pfi_corr,cnn_pfi_corr,lstm_pfi_corr]
models_corr = pd.DataFrame(models_corr)
models_corr = models_corr.transpose()
models_corr.columns = ['GBC','ADA','RFC','NB','NNMLP','SVC','CNN','LSTM']
models_corr

models_mis = [gbc_pfi_mis,ada_pfi_mis,rfc_pfi_mis,nb_pfi_mis,nnmlp_pfi_mis,svc_pfi_mis,cnn_pfi_mis,lstm_pfi_mis]
models_mis = pd.DataFrame(models_mis)
models_mis = models_mis.transpose()
models_mis.columns = ['GBC','ADA','RFC','NB','NNMLP','SVC','CNN','LSTM']
models_mis

corr = models.corr(method='kendall')
rcParams['figure.figsize'] = 8,6
sns.heatmap(corr,annot=True,fmt='0.1g')
plt.title('Kendall Tau Corellations Between Models')
plt.show()

corr_corr = models_corr.corr(method='kendall')
rcParams['figure.figsize'] = 8,6
sns.heatmap(corr_corr,annot=True,fmt='0.1g')
plt.title('Kendall Tau Corellations Between Correct Models')
plt.show()

corr_mis = models_mis.corr(method='kendall')
rcParams['figure.figsize'] = 8,6
sns.heatmap(corr_mis,annot=True,fmt='0.1g')
plt.title('Kendall Tau Corellations Between Misclass Models')
plt.show()

from sklearn.metrics import *
def showResults(test, pred, model_name):
    #target_names = ['positive', 'negative']
    # print(classification_report(test, pred, target_names=target_names))
    accuracy = accuracy_score(test, pred)
    precision= precision_score(test, pred, average='weighted')
    recall = recall_score(test, pred, average = 'weighted')
    f1score= f1_score(test, pred, average='weighted') 
    #loss=log_loss(test,pred)
    print("Accuracy  : {}".format(accuracy))
    print("Precision : {}".format(precision))
    print("Recall : {}".format(recall))
    print("f1score : {}".format(f1score))
    #print("Loss : {}".format(loss))
    cm=confusion_matrix(test, pred,labels=[1,2,3,4,5,6,7,8])
    cm = np.nan_to_num(cm)
    print(cm)
    return (model_name, round(accuracy,3), round(precision,3) , round(recall,3) , round(f1score,3), cm)

cnnpredictions = cnnmodel.predict(X_test1[:,:,:,0], verbose=1)

cnn_predict=np.argmax(cnnpredictions,axis=1)
cnn_actual_value=np.argmax(y_test1,axis=1)
cnn_results = showResults(cnn_actual_value, cnn_predict,'CNN')
from sklearn import metrics
cnnmetrics = metrics.classification_report(cnn_actual_value, cnn_predict, digits=3)
print(cnnmetrics, 'CNN metrics')
categories=['N','L','R','V','A','F','f','/']
plt.figure(figsize=(8,6))
CNN_cf_m = cnn_results[5].astype('float')/ cnn_results[5].sum(axis=1)[:,np.newaxis]
sns.heatmap(np.nan_to_num(CNN_cf_m),annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)
plt.title('1D-CNN confusion matrix')
plt.show()

lstmpredictions = lstmmodel.predict(X_test1[:,:,:,0], verbose=1)

lstm_predict=np.argmax(lstmpredictions,axis=1)
lstm_actual_value=np.argmax(y_test1,axis=1)
lstm_results = showResults(lstm_actual_value, lstm_predict,'LSTM')
from sklearn import metrics
lstmmetrics = metrics.classification_report(lstm_actual_value, lstm_predict, digits=3)
print(lstmmetrics, 'lstm metrics')
categories=['N','L','R','V','A','F','f','/']
plt.figure(figsize=(8,6))
lstm_cf_m = lstm_results[5].astype('float')/ lstm_results[5].sum(axis=1)[:,np.newaxis]
sns.heatmap(np.nan_to_num(lstm_cf_m),annot=True,fmt='.2%',xticklabels=categories,yticklabels=categories,vmin=0,vmax=1)
plt.title('LSTM confusion matrix')
plt.show()